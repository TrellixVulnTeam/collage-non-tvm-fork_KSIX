[14:19:02] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(32, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(32, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(32, 1, 3, 3), float32] {
    reshape(%p0, newshape=[32, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31eeec950 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
[14:19:02] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(96, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(96, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(96, 1, 3, 3), float32] {
    reshape(%p0, newshape=[96, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31e66f950 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
[14:19:02] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(144, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(144, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(144, 1, 3, 3), float32] {
    reshape(%p0, newshape=[144, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31e66f950 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
[14:19:02] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(144, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(144, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(144, 1, 3, 3), float32] {
    reshape(%p0, newshape=[144, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31eeec950 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
[14:19:02] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(192, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(192, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(192, 1, 3, 3), float32] {
    reshape(%p0, newshape=[192, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31eeec950 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
[14:19:03] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(192, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(192, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(192, 1, 3, 3), float32] {
    reshape(%p0, newshape=[192, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31a8ee950 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
[14:19:03] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(192, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(192, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(192, 1, 3, 3), float32] {
    reshape(%p0, newshape=[192, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31a8ee950 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
[14:19:03] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(384, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(384, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(384, 1, 3, 3), float32] {
    reshape(%p0, newshape=[384, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31a8ee950 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
[14:19:03] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(384, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(384, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(384, 1, 3, 3), float32] {
    reshape(%p0, newshape=[384, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31aa56550 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
[14:19:03] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(384, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(384, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(384, 1, 3, 3), float32] {
    reshape(%p0, newshape=[384, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31aa56550 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
[14:19:03] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(384, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(384, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(384, 1, 3, 3), float32] {
    reshape(%p0, newshape=[384, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31aa56550 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
[14:19:03] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(576, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(576, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(576, 1, 3, 3), float32] {
    reshape(%p0, newshape=[576, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31edf4150 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
[14:19:03] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(576, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(576, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(576, 1, 3, 3), float32] {
    reshape(%p0, newshape=[576, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31edf4150 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
[14:19:03] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(576, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(576, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(576, 1, 3, 3), float32] {
    reshape(%p0, newshape=[576, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31e69dd50 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
[14:19:03] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(960, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(960, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(960, 1, 3, 3), float32] {
    reshape(%p0, newshape=[960, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31e69dd50 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
[14:19:03] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(960, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(960, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(960, 1, 3, 3), float32] {
    reshape(%p0, newshape=[960, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31e6bf550 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
[14:19:03] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn () -> Tensor[(960, 1, 3, 3), float32] {
  %0 = fn (%p0: Tensor[(960, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(960, 1, 3, 3), float32] {
    reshape(%p0, newshape=[960, 1, 3, 3])
  };
  %0(meta[relay.Constant][0] /* group=0x55f31e6bf550 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
(Loaded network, Shape array) = (mobilenet_v2, [('input0', [1, 32, 224, 224])])
reshape, llvm -keys=cpu -link-params=0 --> injective.cpu
lower function fused_reshape
primfn(placeholder_1: handle, T_reshape_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_reshape: Buffer(T_reshape_2: Pointer(float32), float32, [32, 1, 3, 3], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [32, 1, 3, 3], [])}
  buffer_map = {placeholder_1: placeholder, T_reshape_1: T_reshape} {
  for (ax0.ax1.fused: int32, 0, 32) "parallel" {
    for (ax2: int32, 0, 3) {
      T_reshape_2[ramp(((ax0.ax1.fused*9) + (ax2*3)), 1, 3)] = (float32x3*)placeholder_2[ramp(((ax0.ax1.fused*9) + (ax2*3)), 1, 3)]
    }
  }
}


reshape, llvm -keys=cpu -link-params=0 --> injective.cpu
lower function fused_reshape_1
primfn(placeholder_1: handle, T_reshape_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_reshape: Buffer(T_reshape_2: Pointer(float32), float32, [96, 1, 3, 3], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [96, 1, 3, 3], [])}
  buffer_map = {placeholder_1: placeholder, T_reshape_1: T_reshape} {
  for (ax0.ax1.fused: int32, 0, 96) "parallel" {
    for (ax2: int32, 0, 3) {
      T_reshape_2[ramp(((ax0.ax1.fused*9) + (ax2*3)), 1, 3)] = (float32x3*)placeholder_2[ramp(((ax0.ax1.fused*9) + (ax2*3)), 1, 3)]
    }
  }
}


reshape, llvm -keys=cpu -link-params=0 --> injective.cpu
lower function fused_reshape_2
primfn(placeholder_1: handle, T_reshape_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_reshape: Buffer(T_reshape_2: Pointer(float32), float32, [144, 1, 3, 3], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [144, 1, 3, 3], [])}
  buffer_map = {placeholder_1: placeholder, T_reshape_1: T_reshape} {
  for (ax0.ax1.fused: int32, 0, 144) "parallel" {
    for (ax2: int32, 0, 3) {
      T_reshape_2[ramp(((ax0.ax1.fused*9) + (ax2*3)), 1, 3)] = (float32x3*)placeholder_2[ramp(((ax0.ax1.fused*9) + (ax2*3)), 1, 3)]
    }
  }
}


reshape, llvm -keys=cpu -link-params=0 --> injective.cpu
lower function fused_reshape_3
primfn(placeholder_1: handle, T_reshape_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_reshape: Buffer(T_reshape_2: Pointer(float32), float32, [192, 1, 3, 3], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [192, 1, 3, 3], [])}
  buffer_map = {placeholder_1: placeholder, T_reshape_1: T_reshape} {
  for (ax0.ax1.fused: int32, 0, 192) "parallel" {
    for (ax2: int32, 0, 3) {
      T_reshape_2[ramp(((ax0.ax1.fused*9) + (ax2*3)), 1, 3)] = (float32x3*)placeholder_2[ramp(((ax0.ax1.fused*9) + (ax2*3)), 1, 3)]
    }
  }
}


reshape, llvm -keys=cpu -link-params=0 --> injective.cpu
lower function fused_reshape_4
primfn(placeholder_1: handle, T_reshape_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_reshape: Buffer(T_reshape_2: Pointer(float32), float32, [384, 1, 3, 3], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [384, 1, 3, 3], [])}
  buffer_map = {placeholder_1: placeholder, T_reshape_1: T_reshape} {
  for (ax0.ax1.fused: int32, 0, 384) "parallel" {
    for (ax2: int32, 0, 3) {
      T_reshape_2[ramp(((ax0.ax1.fused*9) + (ax2*3)), 1, 3)] = (float32x3*)placeholder_2[ramp(((ax0.ax1.fused*9) + (ax2*3)), 1, 3)]
    }
  }
}


reshape, llvm -keys=cpu -link-params=0 --> injective.cpu
lower function fused_reshape_5
primfn(placeholder_1: handle, T_reshape_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_reshape: Buffer(T_reshape_2: Pointer(float32), float32, [576, 1, 3, 3], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [576, 1, 3, 3], [])}
  buffer_map = {placeholder_1: placeholder, T_reshape_1: T_reshape} {
  for (ax0.ax1.fused: int32, 0, 576) "parallel" {
    for (ax2: int32, 0, 3) {
      T_reshape_2[ramp(((ax0.ax1.fused*9) + (ax2*3)), 1, 3)] = (float32x3*)placeholder_2[ramp(((ax0.ax1.fused*9) + (ax2*3)), 1, 3)]
    }
  }
}


reshape, llvm -keys=cpu -link-params=0 --> injective.cpu
lower function fused_reshape_6
primfn(placeholder_1: handle, T_reshape_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_reshape: Buffer(T_reshape_2: Pointer(float32), float32, [960, 1, 3, 3], []),
             placeholder: Buffer(placeholder_2: Pointer(float32), float32, [960, 1, 3, 3], [])}
  buffer_map = {placeholder_1: placeholder, T_reshape_1: T_reshape} {
  for (ax0.ax1.fused: int32, 0, 960) "parallel" {
    for (ax2: int32, 0, 3) {
      T_reshape_2[ramp(((ax0.ax1.fused*9) + (ax2*3)), 1, 3)] = (float32x3*)placeholder_2[ramp(((ax0.ax1.fused*9) + (ax2*3)), 1, 3)]
    }
  }
}


Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('depthwise_conv2d_nchw.cuda', ('TENSOR', (1, 32, 224, 224), 'float32'), ('TENSOR', (32, 1, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 32, 224, 224), 'float32'), ('TENSOR', (16, 32, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 16, 224, 224), 'float32'), ('TENSOR', (96, 16, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('depthwise_conv2d_nchw.cuda', ('TENSOR', (1, 96, 224, 224), 'float32'), ('TENSOR', (96, 1, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 96, 112, 112), 'float32'), ('TENSOR', (24, 96, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 24, 112, 112), 'float32'), ('TENSOR', (144, 24, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('depthwise_conv2d_nchw.cuda', ('TENSOR', (1, 144, 112, 112), 'float32'), ('TENSOR', (144, 1, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 144, 112, 112), 'float32'), ('TENSOR', (24, 144, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('depthwise_conv2d_nchw.cuda', ('TENSOR', (1, 144, 112, 112), 'float32'), ('TENSOR', (144, 1, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 144, 56, 56), 'float32'), ('TENSOR', (32, 144, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 32, 56, 56), 'float32'), ('TENSOR', (192, 32, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('depthwise_conv2d_nchw.cuda', ('TENSOR', (1, 192, 56, 56), 'float32'), ('TENSOR', (192, 1, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 192, 56, 56), 'float32'), ('TENSOR', (32, 192, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('depthwise_conv2d_nchw.cuda', ('TENSOR', (1, 192, 56, 56), 'float32'), ('TENSOR', (192, 1, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 192, 28, 28), 'float32'), ('TENSOR', (64, 192, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 64, 28, 28), 'float32'), ('TENSOR', (384, 64, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('depthwise_conv2d_nchw.cuda', ('TENSOR', (1, 384, 28, 28), 'float32'), ('TENSOR', (384, 1, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 384, 28, 28), 'float32'), ('TENSOR', (64, 384, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 384, 28, 28), 'float32'), ('TENSOR', (96, 384, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 96, 28, 28), 'float32'), ('TENSOR', (576, 96, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('depthwise_conv2d_nchw.cuda', ('TENSOR', (1, 576, 28, 28), 'float32'), ('TENSOR', (576, 1, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 576, 28, 28), 'float32'), ('TENSOR', (96, 576, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('depthwise_conv2d_nchw.cuda', ('TENSOR', (1, 576, 28, 28), 'float32'), ('TENSOR', (576, 1, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 576, 14, 14), 'float32'), ('TENSOR', (160, 576, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 160, 14, 14), 'float32'), ('TENSOR', (960, 160, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('depthwise_conv2d_nchw.cuda', ('TENSOR', (1, 960, 14, 14), 'float32'), ('TENSOR', (960, 1, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 960, 14, 14), 'float32'), ('TENSOR', (160, 960, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 960, 14, 14), 'float32'), ('TENSOR', (320, 960, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_cudnn.cuda', ('TENSOR', (1, 320, 14, 14), 'float32'), ('TENSOR', (1280, 320, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 1, 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
[14:19:06] /home/sunggg/tvm/src/relay/transforms/fuse_ops.cc:1229: Dump of group info:
#[version = "0.0.5"]
fn (%input0: Tensor[(1, 32, 224, 224), float32]) -> Tensor[(1, 1280, 14, 14), float32] {
  %1 = fn (%p0: Tensor[(1, 32, 224, 224), float32], %p1: Tensor[(32, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 32, 224, 224), float32] {
    %0 = nn.conv2d(%p0, %p1, padding=[1, 1, 1, 1], groups=32, channels=32, kernel_size=[3, 3]);
    nn.relu(%0)
  };
  %2 = %1(%input0, meta[relay.Constant][0] /* group=0x55f3644e9578 */);
  %3 = fn (%p01: Tensor[(1, 32, 224, 224), float32], %p11: Tensor[(16, 32, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 16, 224, 224), float32] {
    nn.conv2d(%p01, %p11, padding=[0, 0, 0, 0], channels=16, kernel_size=[1, 1])
  };
  %4 = %3(%2, meta[relay.Constant][1] /* group=0x55f3644e9650 */);
  %6 = fn (%p02: Tensor[(1, 16, 224, 224), float32], %p12: Tensor[(96, 16, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 96, 224, 224), float32] {
    %5 = nn.conv2d(%p02, %p12, padding=[0, 0, 0, 0], channels=96, kernel_size=[1, 1]);
    nn.relu(%5)
  };
  %7 = %6(%4, meta[relay.Constant][2] /* group=0x55f3644e96e0 */);
  %9 = fn (%p03: Tensor[(1, 96, 224, 224), float32], %p13: Tensor[(96, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 96, 112, 112), float32] {
    %8 = nn.conv2d(%p03, %p13, strides=[2, 2], padding=[1, 1, 1, 1], groups=96, channels=96, kernel_size=[3, 3]);
    nn.relu(%8)
  };
  %10 = %9(%7, meta[relay.Constant][3] /* group=0x55f3644e97b8 */);
  %11 = fn (%p04: Tensor[(1, 96, 112, 112), float32], %p14: Tensor[(24, 96, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 24, 112, 112), float32] {
    nn.conv2d(%p04, %p14, padding=[0, 0, 0, 0], channels=24, kernel_size=[1, 1])
  };
  %12 = %11(%10, meta[relay.Constant][4] /* group=0x55f3644e9890 */);
  %14 = fn (%p05: Tensor[(1, 24, 112, 112), float32], %p15: Tensor[(144, 24, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 144, 112, 112), float32] {
    %13 = nn.conv2d(%p05, %p15, padding=[0, 0, 0, 0], channels=144, kernel_size=[1, 1]);
    nn.relu(%13)
  };
  %15 = %14(%12, meta[relay.Constant][5] /* group=0x55f3644e9920 */);
  %17 = fn (%p06: Tensor[(1, 144, 112, 112), float32], %p16: Tensor[(144, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 144, 112, 112), float32] {
    %16 = nn.conv2d(%p06, %p16, padding=[1, 1, 1, 1], groups=144, channels=144, kernel_size=[3, 3]);
    nn.relu(%16)
  };
  %18 = %17(%15, meta[relay.Constant][6] /* group=0x55f3644e99f8 */);
  %20 = fn (%p07: Tensor[(1, 144, 112, 112), float32], %p17: Tensor[(24, 144, 1, 1), float32], %p2: Tensor[(1, 24, 112, 112), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 24, 112, 112), float32] {
    %19 = nn.conv2d(%p07, %p17, padding=[0, 0, 0, 0], channels=24, kernel_size=[1, 1]);
    add(%19, %p2)
  };
  %21 = %20(%18, meta[relay.Constant][7] /* group=0x55f3644e9ad0 */, %12);
  %23 = fn (%p08: Tensor[(1, 24, 112, 112), float32], %p18: Tensor[(144, 24, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 144, 112, 112), float32] {
    %22 = nn.conv2d(%p08, %p18, padding=[0, 0, 0, 0], channels=144, kernel_size=[1, 1]);
    nn.relu(%22)
  };
  %24 = %23(%21, meta[relay.Constant][8] /* group=0x55f3644e9ba8 */);
  %26 = fn (%p09: Tensor[(1, 144, 112, 112), float32], %p19: Tensor[(144, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 144, 56, 56), float32] {
    %25 = nn.conv2d(%p09, %p19, strides=[2, 2], padding=[1, 1, 1, 1], groups=144, channels=144, kernel_size=[3, 3]);
    nn.relu(%25)
  };
  %27 = %26(%24, meta[relay.Constant][9] /* group=0x55f3644ea0a8 */);
  %28 = fn (%p010: Tensor[(1, 144, 56, 56), float32], %p110: Tensor[(32, 144, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 32, 56, 56), float32] {
    nn.conv2d(%p010, %p110, padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1])
  };
  %29 = %28(%27, meta[relay.Constant][10] /* group=0x55f3644ea180 */);
  %31 = fn (%p011: Tensor[(1, 32, 56, 56), float32], %p111: Tensor[(192, 32, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 192, 56, 56), float32] {
    %30 = nn.conv2d(%p011, %p111, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]);
    nn.relu(%30)
  };
  %32 = %31(%29, meta[relay.Constant][11] /* group=0x55f3644ea210 */);
  %34 = fn (%p012: Tensor[(1, 192, 56, 56), float32], %p112: Tensor[(192, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 192, 56, 56), float32] {
    %33 = nn.conv2d(%p012, %p112, padding=[1, 1, 1, 1], groups=192, channels=192, kernel_size=[3, 3]);
    nn.relu(%33)
  };
  %35 = %34(%32, meta[relay.Constant][12] /* group=0x55f3644ea2e8 */);
  %37 = fn (%p013: Tensor[(1, 192, 56, 56), float32], %p113: Tensor[(32, 192, 1, 1), float32], %p21: Tensor[(1, 32, 56, 56), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 32, 56, 56), float32] {
    %36 = nn.conv2d(%p013, %p113, padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1]);
    add(%36, %p21)
  };
  %38 = %37(%35, meta[relay.Constant][13] /* group=0x55f3644ea3c0 */, %29);
  %40 = fn (%p014: Tensor[(1, 32, 56, 56), float32], %p114: Tensor[(192, 32, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 192, 56, 56), float32] {
    %39 = nn.conv2d(%p014, %p114, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]);
    nn.relu(%39)
  };
  %41 = %40(%38, meta[relay.Constant][14] /* group=0x55f3644ea498 */);
  %43 = fn (%p015: Tensor[(1, 192, 56, 56), float32], %p115: Tensor[(192, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 192, 56, 56), float32] {
    %42 = nn.conv2d(%p015, %p115, padding=[1, 1, 1, 1], groups=192, channels=192, kernel_size=[3, 3]);
    nn.relu(%42)
  };
  %44 = %43(%41, meta[relay.Constant][15] /* group=0x55f3644ea570 */);
  %46 = fn (%p016: Tensor[(1, 192, 56, 56), float32], %p116: Tensor[(32, 192, 1, 1), float32], %p22: Tensor[(1, 32, 56, 56), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 32, 56, 56), float32] {
    %45 = nn.conv2d(%p016, %p116, padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1]);
    add(%45, %p22)
  };
  %47 = %46(%44, meta[relay.Constant][16] /* group=0x55f3644ea648 */, %38);
  %49 = fn (%p017: Tensor[(1, 32, 56, 56), float32], %p117: Tensor[(192, 32, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 192, 56, 56), float32] {
    %48 = nn.conv2d(%p017, %p117, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]);
    nn.relu(%48)
  };
  %50 = %49(%47, meta[relay.Constant][17] /* group=0x55f3644ea720 */);
  %52 = fn (%p018: Tensor[(1, 192, 56, 56), float32], %p118: Tensor[(192, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 192, 28, 28), float32] {
    %51 = nn.conv2d(%p018, %p118, strides=[2, 2], padding=[1, 1, 1, 1], groups=192, channels=192, kernel_size=[3, 3]);
    nn.relu(%51)
  };
  %53 = %52(%50, meta[relay.Constant][18] /* group=0x55f3644ea7f8 */);
  %54 = fn (%p019: Tensor[(1, 192, 28, 28), float32], %p119: Tensor[(64, 192, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 64, 28, 28), float32] {
    nn.conv2d(%p019, %p119, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1])
  };
  %55 = %54(%53, meta[relay.Constant][19] /* group=0x55f3644ea8d0 */);
  %57 = fn (%p020: Tensor[(1, 64, 28, 28), float32], %p120: Tensor[(384, 64, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 384, 28, 28), float32] {
    %56 = nn.conv2d(%p020, %p120, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1]);
    nn.relu(%56)
  };
  %58 = %57(%55, meta[relay.Constant][20] /* group=0x55f3644ea960 */);
  %60 = fn (%p021: Tensor[(1, 384, 28, 28), float32], %p121: Tensor[(384, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 384, 28, 28), float32] {
    %59 = nn.conv2d(%p021, %p121, padding=[1, 1, 1, 1], groups=384, channels=384, kernel_size=[3, 3]);
    nn.relu(%59)
  };
  %61 = %60(%58, meta[relay.Constant][21] /* group=0x55f3644eaa38 */);
  %63 = fn (%p022: Tensor[(1, 384, 28, 28), float32], %p122: Tensor[(64, 384, 1, 1), float32], %p23: Tensor[(1, 64, 28, 28), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 64, 28, 28), float32] {
    %62 = nn.conv2d(%p022, %p122, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]);
    add(%62, %p23)
  };
  %64 = %63(%61, meta[relay.Constant][22] /* group=0x55f3644eab10 */, %55);
  %66 = fn (%p023: Tensor[(1, 64, 28, 28), float32], %p123: Tensor[(384, 64, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 384, 28, 28), float32] {
    %65 = nn.conv2d(%p023, %p123, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1]);
    nn.relu(%65)
  };
  %67 = %66(%64, meta[relay.Constant][23] /* group=0x55f3644eabe8 */);
  %69 = fn (%p024: Tensor[(1, 384, 28, 28), float32], %p124: Tensor[(384, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 384, 28, 28), float32] {
    %68 = nn.conv2d(%p024, %p124, padding=[1, 1, 1, 1], groups=384, channels=384, kernel_size=[3, 3]);
    nn.relu(%68)
  };
  %70 = %69(%67, meta[relay.Constant][24] /* group=0x55f3644eacc0 */);
  %72 = fn (%p025: Tensor[(1, 384, 28, 28), float32], %p125: Tensor[(64, 384, 1, 1), float32], %p24: Tensor[(1, 64, 28, 28), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 64, 28, 28), float32] {
    %71 = nn.conv2d(%p025, %p125, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]);
    add(%71, %p24)
  };
  %73 = %72(%70, meta[relay.Constant][25] /* group=0x55f3644ead98 */, %64);
  %75 = fn (%p026: Tensor[(1, 64, 28, 28), float32], %p126: Tensor[(384, 64, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 384, 28, 28), float32] {
    %74 = nn.conv2d(%p026, %p126, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1]);
    nn.relu(%74)
  };
  %76 = %75(%73, meta[relay.Constant][26] /* group=0x55f3644eae70 */);
  %78 = fn (%p027: Tensor[(1, 384, 28, 28), float32], %p127: Tensor[(384, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 384, 28, 28), float32] {
    %77 = nn.conv2d(%p027, %p127, padding=[1, 1, 1, 1], groups=384, channels=384, kernel_size=[3, 3]);
    nn.relu(%77)
  };
  %79 = %78(%76, meta[relay.Constant][27] /* group=0x55f3644eaf48 */);
  %81 = fn (%p028: Tensor[(1, 384, 28, 28), float32], %p128: Tensor[(64, 384, 1, 1), float32], %p25: Tensor[(1, 64, 28, 28), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 64, 28, 28), float32] {
    %80 = nn.conv2d(%p028, %p128, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]);
    add(%80, %p25)
  };
  %82 = %81(%79, meta[relay.Constant][28] /* group=0x55f3644eb020 */, %73);
  %84 = fn (%p029: Tensor[(1, 64, 28, 28), float32], %p129: Tensor[(384, 64, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 384, 28, 28), float32] {
    %83 = nn.conv2d(%p029, %p129, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1]);
    nn.relu(%83)
  };
  %85 = %84(%82, meta[relay.Constant][29] /* group=0x55f3644eb0f8 */);
  %87 = fn (%p030: Tensor[(1, 384, 28, 28), float32], %p130: Tensor[(384, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 384, 28, 28), float32] {
    %86 = nn.conv2d(%p030, %p130, padding=[1, 1, 1, 1], groups=384, channels=384, kernel_size=[3, 3]);
    nn.relu(%86)
  };
  %88 = %87(%85, meta[relay.Constant][30] /* group=0x55f3644eb1d0 */);
  %89 = fn (%p031: Tensor[(1, 384, 28, 28), float32], %p131: Tensor[(96, 384, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 96, 28, 28), float32] {
    nn.conv2d(%p031, %p131, padding=[0, 0, 0, 0], channels=96, kernel_size=[1, 1])
  };
  %90 = %89(%88, meta[relay.Constant][31] /* group=0x55f3644eb2a8 */);
  %92 = fn (%p032: Tensor[(1, 96, 28, 28), float32], %p132: Tensor[(576, 96, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 576, 28, 28), float32] {
    %91 = nn.conv2d(%p032, %p132, padding=[0, 0, 0, 0], channels=576, kernel_size=[1, 1]);
    nn.relu(%91)
  };
  %93 = %92(%90, meta[relay.Constant][32] /* group=0x55f3644eb338 */);
  %95 = fn (%p033: Tensor[(1, 576, 28, 28), float32], %p133: Tensor[(576, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 576, 28, 28), float32] {
    %94 = nn.conv2d(%p033, %p133, padding=[1, 1, 1, 1], groups=576, channels=576, kernel_size=[3, 3]);
    nn.relu(%94)
  };
  %96 = %95(%93, meta[relay.Constant][33] /* group=0x55f3644eb410 */);
  %98 = fn (%p034: Tensor[(1, 576, 28, 28), float32], %p134: Tensor[(96, 576, 1, 1), float32], %p26: Tensor[(1, 96, 28, 28), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 96, 28, 28), float32] {
    %97 = nn.conv2d(%p034, %p134, padding=[0, 0, 0, 0], channels=96, kernel_size=[1, 1]);
    add(%97, %p26)
  };
  %99 = %98(%96, meta[relay.Constant][34] /* group=0x55f3644eb4e8 */, %90);
  %101 = fn (%p035: Tensor[(1, 96, 28, 28), float32], %p135: Tensor[(576, 96, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 576, 28, 28), float32] {
    %100 = nn.conv2d(%p035, %p135, padding=[0, 0, 0, 0], channels=576, kernel_size=[1, 1]);
    nn.relu(%100)
  };
  %102 = %101(%99, meta[relay.Constant][35] /* group=0x55f3644eb5c0 */);
  %104 = fn (%p036: Tensor[(1, 576, 28, 28), float32], %p136: Tensor[(576, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 576, 28, 28), float32] {
    %103 = nn.conv2d(%p036, %p136, padding=[1, 1, 1, 1], groups=576, channels=576, kernel_size=[3, 3]);
    nn.relu(%103)
  };
  %105 = %104(%102, meta[relay.Constant][36] /* group=0x55f3644eb698 */);
  %107 = fn (%p037: Tensor[(1, 576, 28, 28), float32], %p137: Tensor[(96, 576, 1, 1), float32], %p27: Tensor[(1, 96, 28, 28), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 96, 28, 28), float32] {
    %106 = nn.conv2d(%p037, %p137, padding=[0, 0, 0, 0], channels=96, kernel_size=[1, 1]);
    add(%106, %p27)
  };
  %108 = %107(%105, meta[relay.Constant][37] /* group=0x55f3644eb770 */, %99);
  %110 = fn (%p038: Tensor[(1, 96, 28, 28), float32], %p138: Tensor[(576, 96, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 576, 28, 28), float32] {
    %109 = nn.conv2d(%p038, %p138, padding=[0, 0, 0, 0], channels=576, kernel_size=[1, 1]);
    nn.relu(%109)
  };
  %111 = %110(%108, meta[relay.Constant][38] /* group=0x55f3644eb848 */);
  %113 = fn (%p039: Tensor[(1, 576, 28, 28), float32], %p139: Tensor[(576, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 576, 14, 14), float32] {
    %112 = nn.conv2d(%p039, %p139, strides=[2, 2], padding=[1, 1, 1, 1], groups=576, channels=576, kernel_size=[3, 3]);
    nn.relu(%112)
  };
  %114 = %113(%111, meta[relay.Constant][39] /* group=0x55f3644eb920 */);
  %115 = fn (%p040: Tensor[(1, 576, 14, 14), float32], %p140: Tensor[(160, 576, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 160, 14, 14), float32] {
    nn.conv2d(%p040, %p140, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1])
  };
  %116 = %115(%114, meta[relay.Constant][40] /* group=0x55f3644eb9f8 */);
  %118 = fn (%p041: Tensor[(1, 160, 14, 14), float32], %p141: Tensor[(960, 160, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 960, 14, 14), float32] {
    %117 = nn.conv2d(%p041, %p141, padding=[0, 0, 0, 0], channels=960, kernel_size=[1, 1]);
    nn.relu(%117)
  };
  %119 = %118(%116, meta[relay.Constant][41] /* group=0x55f3644eba88 */);
  %121 = fn (%p042: Tensor[(1, 960, 14, 14), float32], %p142: Tensor[(960, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 960, 14, 14), float32] {
    %120 = nn.conv2d(%p042, %p142, padding=[1, 1, 1, 1], groups=960, channels=960, kernel_size=[3, 3]);
    nn.relu(%120)
  };
  %122 = %121(%119, meta[relay.Constant][42] /* group=0x55f3644ebb60 */);
  %124 = fn (%p043: Tensor[(1, 960, 14, 14), float32], %p143: Tensor[(160, 960, 1, 1), float32], %p28: Tensor[(1, 160, 14, 14), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 160, 14, 14), float32] {
    %123 = nn.conv2d(%p043, %p143, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]);
    add(%123, %p28)
  };
  %125 = %124(%122, meta[relay.Constant][43] /* group=0x55f3644ebc38 */, %116);
  %127 = fn (%p044: Tensor[(1, 160, 14, 14), float32], %p144: Tensor[(960, 160, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 960, 14, 14), float32] {
    %126 = nn.conv2d(%p044, %p144, padding=[0, 0, 0, 0], channels=960, kernel_size=[1, 1]);
    nn.relu(%126)
  };
  %128 = %127(%125, meta[relay.Constant][44] /* group=0x55f3644ebd10 */);
  %130 = fn (%p045: Tensor[(1, 960, 14, 14), float32], %p145: Tensor[(960, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 960, 14, 14), float32] {
    %129 = nn.conv2d(%p045, %p145, padding=[1, 1, 1, 1], groups=960, channels=960, kernel_size=[3, 3]);
    nn.relu(%129)
  };
  %131 = %130(%128, meta[relay.Constant][45] /* group=0x55f3644ebde8 */);
  %133 = fn (%p046: Tensor[(1, 960, 14, 14), float32], %p146: Tensor[(160, 960, 1, 1), float32], %p29: Tensor[(1, 160, 14, 14), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 160, 14, 14), float32] {
    %132 = nn.conv2d(%p046, %p146, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]);
    add(%132, %p29)
  };
  %134 = %133(%131, meta[relay.Constant][46] /* group=0x55f3644ebec0 */, %125);
  %136 = fn (%p047: Tensor[(1, 160, 14, 14), float32], %p147: Tensor[(960, 160, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 960, 14, 14), float32] {
    %135 = nn.conv2d(%p047, %p147, padding=[0, 0, 0, 0], channels=960, kernel_size=[1, 1]);
    nn.relu(%135)
  };
  %137 = %136(%134, meta[relay.Constant][47] /* group=0x55f3644ebf98 */);
  %139 = fn (%p048: Tensor[(1, 960, 14, 14), float32], %p148: Tensor[(960, 1, 3, 3), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 960, 14, 14), float32] {
    %138 = nn.conv2d(%p048, %p148, padding=[1, 1, 1, 1], groups=960, channels=960, kernel_size=[3, 3]);
    nn.relu(%138)
  };
  %140 = %139(%137, meta[relay.Constant][48] /* group=0x55f3644ec070 */);
  %141 = fn (%p049: Tensor[(1, 960, 14, 14), float32], %p149: Tensor[(320, 960, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 320, 14, 14), float32] {
    nn.conv2d(%p049, %p149, padding=[0, 0, 0, 0], channels=320, kernel_size=[1, 1])
  };
  %142 = %141(%140, meta[relay.Constant][49] /* group=0x55f3644ec148 */);
  %144 = fn (%p050: Tensor[(1, 320, 14, 14), float32], %p150: Tensor[(1280, 320, 1, 1), float32], Primitive=1, BackendOp="INVALID_BACKEND_OP") -> Tensor[(1, 1280, 14, 14), float32] {
    %143 = nn.conv2d(%p050, %p150, padding=[0, 0, 0, 0], channels=1280, kernel_size=[1, 1]);
    nn.relu(%143)
  };
  %144(%142, meta[relay.Constant][50] /* group=0x55f3644ec1d8 */)
}
/* For debugging purposes the metadata section has been omitted.
 * If you would like to see the full metadata section you can set the 
 * option to `True` when invoking `astext`. 
 */
nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 1280, 14, 14], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [1280, 320, 1, 1], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 320, 14, 14], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [y: Pointer(float32)] "storage_scope" = "global";
  allocate(y, float32, [250880]) {
    attr [0] "extern_scope" = 0;
    @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 0, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_5, @tir.tvm_stack_make_shape(1, 320, 14, 14, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_4, @tir.tvm_stack_make_shape(1280, 320, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y, @tir.tvm_stack_make_shape(1, 1280, 14, 14, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 245;
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 1024;
    T_relu_2[((blockIdx.x*1024) + threadIdx.x)] = max((float32*)y[((blockIdx.x*1024) + threadIdx.x)], 0f32)
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
lower function fused_nn_conv2d
primfn(placeholder_2: handle, placeholder_3: handle, y_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {y: Buffer(y_2: Pointer(float32), float32, [1, 320, 14, 14], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [320, 960, 1, 1], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 960, 14, 14], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, y_1: y} {
  attr [0] "extern_scope" = 0;
  @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 1, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_5, @tir.tvm_stack_make_shape(1, 960, 14, 14, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_4, @tir.tvm_stack_make_shape(320, 960, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y_2, @tir.tvm_stack_make_shape(1, 320, 14, 14, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> depthwise_conv2d_nchw.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu_1
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 960, 14, 14], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [960, 1, 3, 3], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 960, 14, 14], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [IterVar(blockIdx.z: int32, (nullptr), "ThreadIndex", "blockIdx.z")] "thread_extent" = 480;
  attr [PaddedInput.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(PaddedInput.shared, float32, [512]);
  attr [placeholder.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(placeholder.shared, float32, [18]);
  attr [PaddedInput.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(PaddedInput.shared.local, float32, [24]);
  attr [placeholder.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(placeholder.shared.local, float32, [18]);
  attr [DepthwiseConv2d: Pointer(float32)] "storage_scope" = "local";
  allocate(DepthwiseConv2d, float32, [4]);
  attr [IterVar(blockIdx.y: int32, (nullptr), "ThreadIndex", "blockIdx.y")] "thread_extent" = 1;
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1 {
    for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 6) {
      attr [IterVar(threadIdx.z: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
      attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 7;
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 14;
      if @tir.likely(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*14)) + threadIdx.x) < 512), dtype=bool) {
        if @tir.likely((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*7) + threadIdx.y) < 37), dtype=bool) {
          PaddedInput.shared[(((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*14)) + threadIdx.x)] = @tir.if_then_else(((((16 <= floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*14)) + threadIdx.x), 256)) && (floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*14)) + threadIdx.x), 256) < 240)) && (1 <= floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*14)) + threadIdx.x), 16))) && (floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*14)) + threadIdx.x), 16) < 15)), (float32*)placeholder_5[(((((blockIdx.z*392) + (floordiv((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*14)) + threadIdx.x), 256)*196)) + (floordiv(floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*14)) + threadIdx.x), 256), 16)*14)) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*14)) + threadIdx.x), 16)) - 15)], 0f32, dtype=float32)
        }
      }
    }
    attr [IterVar(threadIdx.z_1: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
    attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 7;
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 14;
    if @tir.likely((((threadIdx.y_1*14) + threadIdx.x_1) < 18), dtype=bool) {
      if @tir.likely((threadIdx.y_1 < 2), dtype=bool) {
        placeholder.shared[((threadIdx.y_1*14) + threadIdx.x_1)] = (float32*)placeholder_4[(((blockIdx.z*18) + (threadIdx.y_1*14)) + threadIdx.x_1)]
      }
    }
    attr [IterVar(threadIdx.z_2: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
    attr [IterVar(threadIdx.y_2: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 7;
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 14 {
      for (ax1: int32, 0, 2) {
        for (ax2: int32, 0, 4) {
          for (ax3: int32, 0, 3) {
            PaddedInput.shared.local[(((ax1*12) + (ax2*3)) + ax3)] = (float32*)PaddedInput.shared[(((((ax1*256) + (threadIdx.y_2*32)) + (ax2*16)) + ax3) + threadIdx.x_2)]
          }
        }
      }
      for (ax0: int32, 0, 2) {
        for (ax2_1: int32, 0, 3) {
          for (ax3_1: int32, 0, 3) {
            placeholder.shared.local[(((ax0*9) + (ax2_1*3)) + ax3_1)] = (float32*)placeholder.shared[(((ax0*9) + (ax2_1*3)) + ax3_1)]
          }
        }
      }
      for (c: int32, 0, 2) {
        for (i: int32, 0, 2) {
          DepthwiseConv2d[((c*2) + i)] = 0f32
          for (di: int32, 0, 3) {
            for (dj: int32, 0, 3) {
              DepthwiseConv2d[((c*2) + i)] = ((float32*)DepthwiseConv2d[((c*2) + i)] + ((float32*)PaddedInput.shared.local[((((c*12) + (i*3)) + (di*3)) + dj)]*(float32*)placeholder.shared.local[(((c*9) + (di*3)) + dj)]))
            }
          }
        }
      }
      for (ax1.inner.inner.inner: int32, 0, 2) {
        for (ax2.inner.inner.inner: int32, 0, 2) {
          T_relu_2[(((((blockIdx.z*392) + (ax1.inner.inner.inner*196)) + (threadIdx.y_2*28)) + (ax2.inner.inner.inner*14)) + threadIdx.x_2)] = max((float32*)DepthwiseConv2d[((ax1.inner.inner.inner*2) + ax2.inner.inner.inner)], 0f32)
        }
      }
    }
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu_2
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 960, 14, 14], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [960, 160, 1, 1], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 160, 14, 14], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [y: Pointer(float32)] "storage_scope" = "global";
  allocate(y, float32, [188160]) {
    attr [0] "extern_scope" = 0;
    @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 1, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_5, @tir.tvm_stack_make_shape(1, 160, 14, 14, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_4, @tir.tvm_stack_make_shape(960, 160, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y, @tir.tvm_stack_make_shape(1, 960, 14, 14, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 184;
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 1024;
    if @tir.likely((((blockIdx.x*1024) + threadIdx.x) < 188160), dtype=bool) {
      T_relu_2[((blockIdx.x*1024) + threadIdx.x)] = max((float32*)y[((blockIdx.x*1024) + threadIdx.x)], 0f32)
    }
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
add, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_add
primfn(placeholder_3: handle, placeholder_4: handle, placeholder_5: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [1, 160, 14, 14], []),
             placeholder_2: Buffer(placeholder_6: Pointer(float32), float32, [1, 160, 14, 14], []),
             placeholder_1: Buffer(placeholder_7: Pointer(float32), float32, [160, 960, 1, 1], []),
             placeholder: Buffer(placeholder_8: Pointer(float32), float32, [1, 960, 14, 14], [])}
  buffer_map = {placeholder_3: placeholder, placeholder_4: placeholder_1, placeholder_5: placeholder_2, T_add_1: T_add} {
  attr [y: Pointer(float32)] "storage_scope" = "global";
  allocate(y, float32, [31360]) {
    attr [0] "extern_scope" = 0;
    @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 1, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_8, @tir.tvm_stack_make_shape(1, 960, 14, 14, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_7, @tir.tvm_stack_make_shape(160, 960, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y, @tir.tvm_stack_make_shape(1, 160, 14, 14, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 31;
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 1024;
    if @tir.likely((((blockIdx.x*1024) + threadIdx.x) < 31360), dtype=bool) {
      T_add_2[((blockIdx.x*1024) + threadIdx.x)] = ((float32*)y[((blockIdx.x*1024) + threadIdx.x)] + (float32*)placeholder_6[((blockIdx.x*1024) + threadIdx.x)])
    }
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
lower function fused_nn_conv2d_1
primfn(placeholder_2: handle, placeholder_3: handle, y_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {y: Buffer(y_2: Pointer(float32), float32, [1, 160, 14, 14], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [160, 576, 1, 1], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 576, 14, 14], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, y_1: y} {
  attr [0] "extern_scope" = 0;
  @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 0, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_5, @tir.tvm_stack_make_shape(1, 576, 14, 14, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_4, @tir.tvm_stack_make_shape(160, 576, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y_2, @tir.tvm_stack_make_shape(1, 160, 14, 14, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> depthwise_conv2d_nchw.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu_3
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 576, 14, 14], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [576, 1, 3, 3], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 576, 28, 28], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [IterVar(blockIdx.z: int32, (nullptr), "ThreadIndex", "blockIdx.z")] "thread_extent" = 144;
  attr [PaddedInput.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(PaddedInput.shared, float32, [580]);
  attr [placeholder.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(placeholder.shared, float32, [36]);
  attr [PaddedInput.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(PaddedInput.shared.local, float32, [15]);
  attr [placeholder.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(placeholder.shared.local, float32, [9]);
  attr [DepthwiseConv2d: Pointer(float32)] "storage_scope" = "local";
  allocate(DepthwiseConv2d, float32, [2]);
  attr [IterVar(blockIdx.y: int32, (nullptr), "ThreadIndex", "blockIdx.y")] "thread_extent" = 7;
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1 {
    for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 11) "unroll" {
      attr [IterVar(threadIdx.z: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 4;
      attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2;
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 7;
      if @tir.likely((((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*56) + (threadIdx.z*14)) + (threadIdx.y*7)) + threadIdx.x) < 580), dtype=bool) {
        if @tir.likely(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*8) + (threadIdx.z*2)) + threadIdx.y) < 83), dtype=bool) {
          if @tir.likely((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*4) + threadIdx.z) < 42), dtype=bool) {
            PaddedInput.shared[((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*56) + (threadIdx.z*14)) + (threadIdx.y*7)) + threadIdx.x)] = @tir.if_then_else(((1 <= ((blockIdx.y*4) + floordiv(floormod(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*56) + (threadIdx.z*14)) + (threadIdx.y*7)) + threadIdx.x), 145), 29))) && (1 <= floormod(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*56) + (threadIdx.z*14)) + (threadIdx.y*7)) + threadIdx.x), 29))), (float32*)placeholder_5[((((((blockIdx.z*3136) + (floordiv(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*56) + (threadIdx.z*14)) + (threadIdx.y*7)) + threadIdx.x), 145)*784)) + (blockIdx.y*112)) + (floordiv(floormod(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*56) + (threadIdx.z*14)) + (threadIdx.y*7)) + threadIdx.x), 145), 29)*28)) + floormod(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*56) + (threadIdx.z*14)) + (threadIdx.y*7)) + threadIdx.x), 29)) - 29)], 0f32, dtype=float32)
          }
        }
      }
    }
    attr [IterVar(threadIdx.z_1: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 4;
    attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2;
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 7;
    if @tir.likely(((((threadIdx.z_1*14) + (threadIdx.y_1*7)) + threadIdx.x_1) < 36), dtype=bool) {
      if @tir.likely((((threadIdx.z_1*2) + threadIdx.y_1) < 6), dtype=bool) {
        if @tir.likely((threadIdx.z_1 < 3), dtype=bool) {
          placeholder.shared[(((threadIdx.z_1*14) + (threadIdx.y_1*7)) + threadIdx.x_1)] = (float32*)placeholder_4[((((blockIdx.z*36) + (threadIdx.z_1*14)) + (threadIdx.y_1*7)) + threadIdx.x_1)]
        }
      }
    }
    attr [IterVar(threadIdx.z_2: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 4;
    attr [IterVar(threadIdx.y_2: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2;
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 7 {
      for (ax2: int32, 0, 3) "unroll" {
        for (ax3: int32, 0, 5) "unroll" {
          PaddedInput.shared.local[((ax2*5) + ax3)] = (float32*)PaddedInput.shared[(((((threadIdx.z_2*145) + (threadIdx.y_2*58)) + (ax2*29)) + (threadIdx.x_2*4)) + ax3)]
        }
      }
      for (ax2_1: int32, 0, 3) "unroll" {
        for (ax3_1: int32, 0, 3) "unroll" {
          placeholder.shared.local[((ax2_1*3) + ax3_1)] = (float32*)placeholder.shared[(((threadIdx.z_2*9) + (ax2_1*3)) + ax3_1)]
        }
      }
      for (j: int32, 0, 2) "unroll" {
        DepthwiseConv2d[j] = 0f32
        for (di: int32, 0, 3) "unroll" {
          for (dj: int32, 0, 3) "unroll" {
            DepthwiseConv2d[j] = ((float32*)DepthwiseConv2d[j] + ((float32*)PaddedInput.shared.local[(((di*5) + (j*2)) + dj)]*(float32*)placeholder.shared.local[((di*3) + dj)]))
          }
        }
      }
      for (ax3.inner.inner.inner: int32, 0, 2) "unroll" {
        T_relu_2[((((((blockIdx.z*784) + (threadIdx.z_2*196)) + (blockIdx.y*28)) + (threadIdx.y_2*14)) + (threadIdx.x_2*2)) + ax3.inner.inner.inner)] = max((float32*)DepthwiseConv2d[ax3.inner.inner.inner], 0f32)
      }
    }
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu_4
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 576, 28, 28], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [576, 96, 1, 1], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 96, 28, 28], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [y: Pointer(float32)] "storage_scope" = "global";
  allocate(y, float32, [451584]) {
    attr [0] "extern_scope" = 0;
    @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 1, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_5, @tir.tvm_stack_make_shape(1, 96, 28, 28, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_4, @tir.tvm_stack_make_shape(576, 96, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y, @tir.tvm_stack_make_shape(1, 576, 28, 28, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 441;
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 1024;
    T_relu_2[((blockIdx.x*1024) + threadIdx.x)] = max((float32*)y[((blockIdx.x*1024) + threadIdx.x)], 0f32)
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
add, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_add_1
primfn(placeholder_3: handle, placeholder_4: handle, placeholder_5: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [1, 96, 28, 28], []),
             placeholder_2: Buffer(placeholder_6: Pointer(float32), float32, [1, 96, 28, 28], []),
             placeholder_1: Buffer(placeholder_7: Pointer(float32), float32, [96, 576, 1, 1], []),
             placeholder: Buffer(placeholder_8: Pointer(float32), float32, [1, 576, 28, 28], [])}
  buffer_map = {placeholder_3: placeholder, placeholder_4: placeholder_1, placeholder_5: placeholder_2, T_add_1: T_add} {
  attr [y: Pointer(float32)] "storage_scope" = "global";
  allocate(y, float32, [75264]) {
    attr [0] "extern_scope" = 0;
    @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 0, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_8, @tir.tvm_stack_make_shape(1, 576, 28, 28, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_7, @tir.tvm_stack_make_shape(96, 576, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y, @tir.tvm_stack_make_shape(1, 96, 28, 28, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 74;
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 1024;
    if @tir.likely((((blockIdx.x*1024) + threadIdx.x) < 75264), dtype=bool) {
      T_add_2[((blockIdx.x*1024) + threadIdx.x)] = ((float32*)y[((blockIdx.x*1024) + threadIdx.x)] + (float32*)placeholder_6[((blockIdx.x*1024) + threadIdx.x)])
    }
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> depthwise_conv2d_nchw.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu_5
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 576, 28, 28], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [576, 1, 3, 3], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 576, 28, 28], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [IterVar(blockIdx.z: int32, (nullptr), "ThreadIndex", "blockIdx.z")] "thread_extent" = 576;
  attr [PaddedInput.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(PaddedInput.shared, float32, [270]);
  attr [placeholder.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(placeholder.shared, float32, [9]);
  attr [PaddedInput.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(PaddedInput.shared.local, float32, [12]);
  attr [placeholder.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(placeholder.shared.local, float32, [9]);
  attr [DepthwiseConv2d: Pointer(float32)] "storage_scope" = "local";
  allocate(DepthwiseConv2d, float32, [2]);
  attr [IterVar(blockIdx.y: int32, (nullptr), "ThreadIndex", "blockIdx.y")] "thread_extent" = 1;
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4 {
    for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 3) "unroll" {
      attr [IterVar(threadIdx.z: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
      attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 14;
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 7;
      if @tir.likely(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*7)) + threadIdx.x) < 270), dtype=bool) {
        if @tir.likely((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*14) + threadIdx.y) < 39), dtype=bool) {
          PaddedInput.shared[(((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*7)) + threadIdx.x)] = @tir.if_then_else(((((9 <= (((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*7)) + threadIdx.x)) && ((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*7)) + threadIdx.x) < 261)) && (1 <= ((blockIdx.x*7) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*7)) + threadIdx.x), 9)))) && (((blockIdx.x*7) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*7)) + threadIdx.x), 9)) < 29)), (float32*)placeholder_5[(((((blockIdx.z*784) + (floordiv((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*7)) + threadIdx.x), 9)*28)) + (blockIdx.x*7)) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*98) + (threadIdx.y*7)) + threadIdx.x), 9)) - 29)], 0f32, dtype=float32)
        }
      }
    }
    attr [IterVar(threadIdx.z_1: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
    attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 14;
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 7;
    if @tir.likely((((threadIdx.y_1*7) + threadIdx.x_1) < 9), dtype=bool) {
      if @tir.likely((threadIdx.y_1 < 2), dtype=bool) {
        placeholder.shared[((threadIdx.y_1*7) + threadIdx.x_1)] = (float32*)placeholder_4[(((blockIdx.z*9) + (threadIdx.y_1*7)) + threadIdx.x_1)]
      }
    }
    attr [IterVar(threadIdx.z_2: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
    attr [IterVar(threadIdx.y_2: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 14;
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 7 {
      for (ax2: int32, 0, 4) "unroll" {
        for (ax3: int32, 0, 3) "unroll" {
          PaddedInput.shared.local[((ax2*3) + ax3)] = (float32*)PaddedInput.shared[((((threadIdx.y_2*18) + (ax2*9)) + ax3) + threadIdx.x_2)]
        }
      }
      for (ax2_1: int32, 0, 3) "unroll" {
        for (ax3_1: int32, 0, 3) "unroll" {
          placeholder.shared.local[((ax2_1*3) + ax3_1)] = (float32*)placeholder.shared[((ax2_1*3) + ax3_1)]
        }
      }
      for (i: int32, 0, 2) "unroll" {
        DepthwiseConv2d[i] = 0f32
        for (di: int32, 0, 3) "unroll" {
          for (dj: int32, 0, 3) "unroll" {
            DepthwiseConv2d[i] = ((float32*)DepthwiseConv2d[i] + ((float32*)PaddedInput.shared.local[(((i*3) + (di*3)) + dj)]*(float32*)placeholder.shared.local[((di*3) + dj)]))
          }
        }
      }
      for (ax2.inner.inner.inner: int32, 0, 2) "unroll" {
        T_relu_2[(((((blockIdx.z*784) + (threadIdx.y_2*56)) + (ax2.inner.inner.inner*28)) + (blockIdx.x*7)) + threadIdx.x_2)] = max((float32*)DepthwiseConv2d[ax2.inner.inner.inner], 0f32)
      }
    }
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
lower function fused_nn_conv2d_2
primfn(placeholder_2: handle, placeholder_3: handle, y_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {y: Buffer(y_2: Pointer(float32), float32, [1, 96, 28, 28], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [96, 384, 1, 1], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 384, 28, 28], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, y_1: y} {
  attr [0] "extern_scope" = 0;
  @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 0, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_5, @tir.tvm_stack_make_shape(1, 384, 28, 28, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_4, @tir.tvm_stack_make_shape(96, 384, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y_2, @tir.tvm_stack_make_shape(1, 96, 28, 28, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> depthwise_conv2d_nchw.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu_6
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 384, 28, 28], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [384, 1, 3, 3], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 384, 28, 28], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [IterVar(blockIdx.z: int32, (nullptr), "ThreadIndex", "blockIdx.z")] "thread_extent" = 192;
  attr [PaddedInput.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(PaddedInput.shared, float32, [960]);
  attr [placeholder.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(placeholder.shared, float32, [18]);
  attr [PaddedInput.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(PaddedInput.shared.local, float32, [24]);
  attr [placeholder.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(placeholder.shared.local, float32, [18]);
  attr [DepthwiseConv2d: Pointer(float32)] "storage_scope" = "local";
  allocate(DepthwiseConv2d, float32, [4]);
  attr [IterVar(blockIdx.y: int32, (nullptr), "ThreadIndex", "blockIdx.y")] "thread_extent" = 2;
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1 {
    for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 5) {
      attr [IterVar(threadIdx.z: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
      attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 7;
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28;
      if @tir.likely(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*196) + (threadIdx.y*28)) + threadIdx.x) < 960), dtype=bool) {
        PaddedInput.shared[(((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*196) + (threadIdx.y*28)) + threadIdx.x)] = @tir.if_then_else(((((1 <= ((blockIdx.y*14) + floordiv(floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*196) + (threadIdx.y*28)) + threadIdx.x), 480), 30))) && (((blockIdx.y*14) + floordiv(floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*196) + (threadIdx.y*28)) + threadIdx.x), 480), 30)) < 29)) && (1 <= floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*196) + (threadIdx.y*28)) + threadIdx.x), 30))) && (floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*196) + (threadIdx.y*28)) + threadIdx.x), 30) < 29)), (float32*)placeholder_5[((((((blockIdx.z*1568) + (floordiv((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*196) + (threadIdx.y*28)) + threadIdx.x), 480)*784)) + (blockIdx.y*392)) + (floordiv(floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*196) + (threadIdx.y*28)) + threadIdx.x), 480), 30)*28)) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*196) + (threadIdx.y*28)) + threadIdx.x), 30)) - 29)], 0f32, dtype=float32)
      }
    }
    attr [IterVar(threadIdx.z_1: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
    attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 7;
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28;
    if @tir.likely((((threadIdx.y_1*28) + threadIdx.x_1) < 18), dtype=bool) {
      if @tir.likely((threadIdx.y_1 < 1), dtype=bool) {
        placeholder.shared[((threadIdx.y_1*28) + threadIdx.x_1)] = (float32*)placeholder_4[(((threadIdx.y_1*28) + (blockIdx.z*18)) + threadIdx.x_1)]
      }
    }
    attr [IterVar(threadIdx.z_2: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
    attr [IterVar(threadIdx.y_2: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 7;
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
      for (ax1: int32, 0, 2) {
        for (ax2: int32, 0, 4) {
          for (ax3: int32, 0, 3) {
            PaddedInput.shared.local[(((ax1*12) + (ax2*3)) + ax3)] = (float32*)PaddedInput.shared[(((((ax1*480) + (threadIdx.y_2*60)) + (ax2*30)) + ax3) + threadIdx.x_2)]
          }
        }
      }
      for (ax0: int32, 0, 2) {
        for (ax2_1: int32, 0, 3) {
          for (ax3_1: int32, 0, 3) {
            placeholder.shared.local[(((ax0*9) + (ax2_1*3)) + ax3_1)] = (float32*)placeholder.shared[(((ax0*9) + (ax2_1*3)) + ax3_1)]
          }
        }
      }
      for (c: int32, 0, 2) {
        for (i: int32, 0, 2) {
          DepthwiseConv2d[((c*2) + i)] = 0f32
          for (di: int32, 0, 3) {
            for (dj: int32, 0, 3) {
              DepthwiseConv2d[((c*2) + i)] = ((float32*)DepthwiseConv2d[((c*2) + i)] + ((float32*)PaddedInput.shared.local[((((c*12) + (i*3)) + (di*3)) + dj)]*(float32*)placeholder.shared.local[(((c*9) + (di*3)) + dj)]))
            }
          }
        }
      }
      for (ax1.inner.inner.inner: int32, 0, 2) {
        for (ax2.inner.inner.inner: int32, 0, 2) {
          T_relu_2[((((((blockIdx.z*1568) + (ax1.inner.inner.inner*784)) + (blockIdx.y*392)) + (threadIdx.y_2*56)) + (ax2.inner.inner.inner*28)) + threadIdx.x_2)] = max((float32*)DepthwiseConv2d[((ax1.inner.inner.inner*2) + ax2.inner.inner.inner)], 0f32)
        }
      }
    }
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu_7
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 384, 28, 28], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [384, 64, 1, 1], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 64, 28, 28], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [y: Pointer(float32)] "storage_scope" = "global";
  allocate(y, float32, [301056]) {
    attr [0] "extern_scope" = 0;
    @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 0, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_5, @tir.tvm_stack_make_shape(1, 64, 28, 28, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_4, @tir.tvm_stack_make_shape(384, 64, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y, @tir.tvm_stack_make_shape(1, 384, 28, 28, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 294;
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 1024;
    T_relu_2[((blockIdx.x*1024) + threadIdx.x)] = max((float32*)y[((blockIdx.x*1024) + threadIdx.x)], 0f32)
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
add, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_add_2
primfn(placeholder_3: handle, placeholder_4: handle, placeholder_5: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [1, 64, 28, 28], []),
             placeholder_2: Buffer(placeholder_6: Pointer(float32), float32, [1, 64, 28, 28], []),
             placeholder_1: Buffer(placeholder_7: Pointer(float32), float32, [64, 384, 1, 1], []),
             placeholder: Buffer(placeholder_8: Pointer(float32), float32, [1, 384, 28, 28], [])}
  buffer_map = {placeholder_3: placeholder, placeholder_4: placeholder_1, placeholder_5: placeholder_2, T_add_1: T_add} {
  attr [y: Pointer(float32)] "storage_scope" = "global";
  allocate(y, float32, [50176]) {
    attr [0] "extern_scope" = 0;
    @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 0, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_8, @tir.tvm_stack_make_shape(1, 384, 28, 28, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_7, @tir.tvm_stack_make_shape(64, 384, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y, @tir.tvm_stack_make_shape(1, 64, 28, 28, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 49;
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 1024;
    T_add_2[((blockIdx.x*1024) + threadIdx.x)] = ((float32*)y[((blockIdx.x*1024) + threadIdx.x)] + (float32*)placeholder_6[((blockIdx.x*1024) + threadIdx.x)])
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
lower function fused_nn_conv2d_3
primfn(placeholder_2: handle, placeholder_3: handle, y_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {y: Buffer(y_2: Pointer(float32), float32, [1, 64, 28, 28], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [64, 192, 1, 1], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 192, 28, 28], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, y_1: y} {
  attr [0] "extern_scope" = 0;
  @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 0, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_5, @tir.tvm_stack_make_shape(1, 192, 28, 28, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_4, @tir.tvm_stack_make_shape(64, 192, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y_2, @tir.tvm_stack_make_shape(1, 64, 28, 28, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> depthwise_conv2d_nchw.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu_8
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 192, 28, 28], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [192, 1, 3, 3], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 192, 56, 56], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [IterVar(blockIdx.z: int32, (nullptr), "ThreadIndex", "blockIdx.z")] "thread_extent" = 96;
  attr [PaddedInput.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(PaddedInput.shared, float32, [3306]);
  attr [placeholder.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(placeholder.shared, float32, [18]);
  attr [PaddedInput.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(PaddedInput.shared.local, float32, [30]);
  attr [placeholder.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(placeholder.shared.local, float32, [18]);
  attr [DepthwiseConv2d: Pointer(float32)] "storage_scope" = "local";
  allocate(DepthwiseConv2d, float32, [4]);
  attr [IterVar(blockIdx.y: int32, (nullptr), "ThreadIndex", "blockIdx.y")] "thread_extent" = 2;
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1 {
    for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 17) {
      attr [IterVar(threadIdx.z: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
      attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 7;
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28;
      if @tir.likely(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*196) + (threadIdx.y*28)) + threadIdx.x) < 3306), dtype=bool) {
        PaddedInput.shared[(((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*196) + (threadIdx.y*28)) + threadIdx.x)] = @tir.if_then_else(((1 <= ((blockIdx.y*28) + floordiv(floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*196) + (threadIdx.y*28)) + threadIdx.x), 1653), 57))) && (1 <= floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*196) + (threadIdx.y*28)) + threadIdx.x), 57))), (float32*)placeholder_5[((((((blockIdx.z*6272) + (floordiv((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*196) + (threadIdx.y*28)) + threadIdx.x), 1653)*3136)) + (blockIdx.y*1568)) + (floordiv(floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*196) + (threadIdx.y*28)) + threadIdx.x), 1653), 57)*56)) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*196) + (threadIdx.y*28)) + threadIdx.x), 57)) - 57)], 0f32, dtype=float32)
      }
    }
    attr [IterVar(threadIdx.z_1: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
    attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 7;
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28;
    if @tir.likely((((threadIdx.y_1*28) + threadIdx.x_1) < 18), dtype=bool) {
      if @tir.likely((threadIdx.y_1 < 1), dtype=bool) {
        placeholder.shared[((threadIdx.y_1*28) + threadIdx.x_1)] = (float32*)placeholder_4[(((threadIdx.y_1*28) + (blockIdx.z*18)) + threadIdx.x_1)]
      }
    }
    attr [IterVar(threadIdx.z_2: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
    attr [IterVar(threadIdx.y_2: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 7;
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
      for (ax1: int32, 0, 2) {
        for (ax2: int32, 0, 5) {
          for (ax3: int32, 0, 3) {
            PaddedInput.shared.local[(((ax1*15) + (ax2*3)) + ax3)] = (float32*)PaddedInput.shared[(((((ax1*1653) + (threadIdx.y_2*228)) + (ax2*57)) + (threadIdx.x_2*2)) + ax3)]
          }
        }
      }
      for (ax0: int32, 0, 2) {
        for (ax2_1: int32, 0, 3) {
          for (ax3_1: int32, 0, 3) {
            placeholder.shared.local[(((ax0*9) + (ax2_1*3)) + ax3_1)] = (float32*)placeholder.shared[(((ax0*9) + (ax2_1*3)) + ax3_1)]
          }
        }
      }
      for (c: int32, 0, 2) {
        for (i: int32, 0, 2) {
          DepthwiseConv2d[((c*2) + i)] = 0f32
          for (di: int32, 0, 3) {
            for (dj: int32, 0, 3) {
              DepthwiseConv2d[((c*2) + i)] = ((float32*)DepthwiseConv2d[((c*2) + i)] + ((float32*)PaddedInput.shared.local[((((c*15) + (i*6)) + (di*3)) + dj)]*(float32*)placeholder.shared.local[(((c*9) + (di*3)) + dj)]))
            }
          }
        }
      }
      for (ax1.inner.inner.inner: int32, 0, 2) {
        for (ax2.inner.inner.inner: int32, 0, 2) {
          T_relu_2[((((((blockIdx.z*1568) + (ax1.inner.inner.inner*784)) + (blockIdx.y*392)) + (threadIdx.y_2*56)) + (ax2.inner.inner.inner*28)) + threadIdx.x_2)] = max((float32*)DepthwiseConv2d[((ax1.inner.inner.inner*2) + ax2.inner.inner.inner)], 0f32)
        }
      }
    }
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu_9
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 192, 56, 56], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [192, 32, 1, 1], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 32, 56, 56], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [y: Pointer(float32)] "storage_scope" = "global";
  allocate(y, float32, [602112]) {
    attr [0] "extern_scope" = 0;
    @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 1, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_5, @tir.tvm_stack_make_shape(1, 32, 56, 56, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_4, @tir.tvm_stack_make_shape(192, 32, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y, @tir.tvm_stack_make_shape(1, 192, 56, 56, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 588;
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 1024;
    T_relu_2[((blockIdx.x*1024) + threadIdx.x)] = max((float32*)y[((blockIdx.x*1024) + threadIdx.x)], 0f32)
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
add, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_add_3
primfn(placeholder_3: handle, placeholder_4: handle, placeholder_5: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_add: Buffer(T_add_2: Pointer(float32), float32, [1, 32, 56, 56], []),
             placeholder_2: Buffer(placeholder_6: Pointer(float32), float32, [1, 32, 56, 56], []),
             placeholder_1: Buffer(placeholder_7: Pointer(float32), float32, [32, 192, 1, 1], []),
             placeholder: Buffer(placeholder_8: Pointer(float32), float32, [1, 192, 56, 56], [])}
  buffer_map = {placeholder_3: placeholder, placeholder_4: placeholder_1, placeholder_5: placeholder_2, T_add_1: T_add} {
  attr [y: Pointer(float32)] "storage_scope" = "global";
  allocate(y, float32, [100352]) {
    attr [0] "extern_scope" = 0;
    @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 0, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_8, @tir.tvm_stack_make_shape(1, 192, 56, 56, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_7, @tir.tvm_stack_make_shape(32, 192, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y, @tir.tvm_stack_make_shape(1, 32, 56, 56, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 98;
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 1024;
    T_add_2[((blockIdx.x*1024) + threadIdx.x)] = ((float32*)y[((blockIdx.x*1024) + threadIdx.x)] + (float32*)placeholder_6[((blockIdx.x*1024) + threadIdx.x)])
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> depthwise_conv2d_nchw.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu_10
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 192, 56, 56], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [192, 1, 3, 3], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 192, 56, 56], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [IterVar(blockIdx.z: int32, (nullptr), "ThreadIndex", "blockIdx.z")] "thread_extent" = 96;
  attr [PaddedInput.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(PaddedInput.shared, float32, [3480]);
  attr [placeholder.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(placeholder.shared, float32, [18]);
  attr [PaddedInput.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(PaddedInput.shared.local, float32, [108]);
  attr [placeholder.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(placeholder.shared.local, float32, [18]);
  attr [DepthwiseConv2d: Pointer(float32)] "storage_scope" = "local";
  allocate(DepthwiseConv2d, float32, [28]);
  attr [IterVar(blockIdx.y: int32, (nullptr), "ThreadIndex", "blockIdx.y")] "thread_extent" = 2;
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1 {
    for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 32) "unroll" {
      attr [IterVar(threadIdx.z: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
      attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2;
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 56;
      if @tir.likely(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*112) + (threadIdx.y*56)) + threadIdx.x) < 3480), dtype=bool) {
        if @tir.likely((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*2) + threadIdx.y) < 63), dtype=bool) {
          PaddedInput.shared[(((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*112) + (threadIdx.y*56)) + threadIdx.x)] = @tir.if_then_else(((((1 <= ((blockIdx.y*28) + floordiv(floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*112) + (threadIdx.y*56)) + threadIdx.x), 1740), 58))) && (((blockIdx.y*28) + floordiv(floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*112) + (threadIdx.y*56)) + threadIdx.x), 1740), 58)) < 57)) && (1 <= floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*112) + (threadIdx.y*56)) + threadIdx.x), 58))) && (floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*112) + (threadIdx.y*56)) + threadIdx.x), 58) < 57)), (float32*)placeholder_5[((((((blockIdx.z*6272) + (floordiv((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*112) + (threadIdx.y*56)) + threadIdx.x), 1740)*3136)) + (blockIdx.y*1568)) + (floordiv(floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*112) + (threadIdx.y*56)) + threadIdx.x), 1740), 58)*56)) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*112) + (threadIdx.y*56)) + threadIdx.x), 58)) - 57)], 0f32, dtype=float32)
        }
      }
    }
    attr [IterVar(threadIdx.z_1: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
    attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2;
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 56;
    if @tir.likely((((threadIdx.y_1*56) + threadIdx.x_1) < 18), dtype=bool) {
      if @tir.likely((threadIdx.y_1 < 1), dtype=bool) {
        placeholder.shared[((threadIdx.y_1*56) + threadIdx.x_1)] = (float32*)placeholder_4[(((threadIdx.y_1*56) + (blockIdx.z*18)) + threadIdx.x_1)]
      }
    }
    attr [IterVar(threadIdx.z_2: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
    attr [IterVar(threadIdx.y_2: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 2;
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 56 {
      for (ax2: int32, 0, 9) "unroll" {
        for (ax3: int32, 0, 3) "unroll" {
          PaddedInput.shared.local[((ax2*3) + ax3)] = (float32*)PaddedInput.shared[((((threadIdx.y_2*406) + (ax2*58)) + ax3) + threadIdx.x_2)]
          PaddedInput.shared.local[(((ax2*3) + ax3) + 54)] = (float32*)PaddedInput.shared[(((((threadIdx.y_2*406) + (ax2*58)) + ax3) + threadIdx.x_2) + 1740)]
          PaddedInput.shared.local[(((ax2*3) + ax3) + 27)] = (float32*)PaddedInput.shared[(((((threadIdx.y_2*406) + (ax2*58)) + ax3) + threadIdx.x_2) + 812)]
          PaddedInput.shared.local[(((ax2*3) + ax3) + 81)] = (float32*)PaddedInput.shared[(((((threadIdx.y_2*406) + (ax2*58)) + ax3) + threadIdx.x_2) + 2552)]
        }
      }
      for (ax2_1: int32, 0, 3) "unroll" {
        for (ax3_1: int32, 0, 3) "unroll" {
          placeholder.shared.local[((ax2_1*3) + ax3_1)] = (float32*)placeholder.shared[((ax2_1*3) + ax3_1)]
          placeholder.shared.local[(((ax2_1*3) + ax3_1) + 9)] = (float32*)placeholder.shared[(((ax2_1*3) + ax3_1) + 9)]
        }
      }
      for (i: int32, 0, 7) {
        DepthwiseConv2d[i] = 0f32
        DepthwiseConv2d[(i + 14)] = 0f32
        DepthwiseConv2d[(i + 7)] = 0f32
        DepthwiseConv2d[(i + 21)] = 0f32
        for (di: int32, 0, 3) "unroll" {
          for (dj: int32, 0, 3) "unroll" {
            DepthwiseConv2d[i] = ((float32*)DepthwiseConv2d[i] + ((float32*)PaddedInput.shared.local[(((i*3) + (di*3)) + dj)]*(float32*)placeholder.shared.local[((di*3) + dj)]))
            DepthwiseConv2d[(i + 14)] = ((float32*)DepthwiseConv2d[(i + 14)] + ((float32*)PaddedInput.shared.local[((((i*3) + (di*3)) + dj) + 54)]*(float32*)placeholder.shared.local[(((di*3) + dj) + 9)]))
            DepthwiseConv2d[(i + 7)] = ((float32*)DepthwiseConv2d[(i + 7)] + ((float32*)PaddedInput.shared.local[((((i*3) + (di*3)) + dj) + 27)]*(float32*)placeholder.shared.local[((di*3) + dj)]))
            DepthwiseConv2d[(i + 21)] = ((float32*)DepthwiseConv2d[(i + 21)] + ((float32*)PaddedInput.shared.local[((((i*3) + (di*3)) + dj) + 81)]*(float32*)placeholder.shared.local[(((di*3) + dj) + 9)]))
          }
        }
      }
      for (ax2.inner.inner.inner: int32, 0, 7) "unroll" {
        T_relu_2[(((((blockIdx.z*6272) + (blockIdx.y*1568)) + (threadIdx.y_2*392)) + (ax2.inner.inner.inner*56)) + threadIdx.x_2)] = max((float32*)DepthwiseConv2d[ax2.inner.inner.inner], 0f32)
        T_relu_2[((((((blockIdx.z*6272) + (blockIdx.y*1568)) + (threadIdx.y_2*392)) + (ax2.inner.inner.inner*56)) + threadIdx.x_2) + 3136)] = max((float32*)DepthwiseConv2d[(ax2.inner.inner.inner + 14)], 0f32)
        T_relu_2[((((((blockIdx.z*6272) + (blockIdx.y*1568)) + (threadIdx.y_2*392)) + (ax2.inner.inner.inner*56)) + threadIdx.x_2) + 784)] = max((float32*)DepthwiseConv2d[(ax2.inner.inner.inner + 7)], 0f32)
        T_relu_2[((((((blockIdx.z*6272) + (blockIdx.y*1568)) + (threadIdx.y_2*392)) + (ax2.inner.inner.inner*56)) + threadIdx.x_2) + 3920)] = max((float32*)DepthwiseConv2d[(ax2.inner.inner.inner + 21)], 0f32)
      }
    }
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
lower function fused_nn_conv2d_4
primfn(placeholder_2: handle, placeholder_3: handle, y_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {y: Buffer(y_2: Pointer(float32), float32, [1, 32, 56, 56], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [32, 144, 1, 1], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 144, 56, 56], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, y_1: y} {
  attr [0] "extern_scope" = 0;
  @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 0, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_5, @tir.tvm_stack_make_shape(1, 144, 56, 56, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_4, @tir.tvm_stack_make_shape(32, 144, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y_2, @tir.tvm_stack_make_shape(1, 32, 56, 56, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> depthwise_conv2d_nchw.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu_11
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 144, 56, 56], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [144, 1, 3, 3], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 144, 112, 112], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [IterVar(blockIdx.z: int32, (nullptr), "ThreadIndex", "blockIdx.z")] "thread_extent" = 36;
  attr [PaddedInput.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(PaddedInput.shared, float32, [3876]);
  attr [placeholder.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(placeholder.shared, float32, [36]);
  attr [PaddedInput.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(PaddedInput.shared.local, float32, [36]);
  attr [placeholder.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(placeholder.shared.local, float32, [36]);
  attr [DepthwiseConv2d: Pointer(float32)] "storage_scope" = "local";
  allocate(DepthwiseConv2d, float32, [4]);
  attr [IterVar(blockIdx.y: int32, (nullptr), "ThreadIndex", "blockIdx.y")] "thread_extent" = 2;
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 7 {
    for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 18) {
      attr [IterVar(threadIdx.z: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
      attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 28;
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8;
      if @tir.likely(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*224) + (threadIdx.y*8)) + threadIdx.x) < 3876), dtype=bool) {
        if @tir.likely((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*28) + threadIdx.y) < 485), dtype=bool) {
          PaddedInput.shared[(((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*224) + (threadIdx.y*8)) + threadIdx.x)] = @tir.if_then_else(((1 <= ((blockIdx.y*56) + floordiv(floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*224) + (threadIdx.y*8)) + threadIdx.x), 969), 17))) && (1 <= ((blockIdx.x*16) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*224) + (threadIdx.y*8)) + threadIdx.x), 17)))), (float32*)placeholder_5[(((((((blockIdx.z*50176) + (floordiv((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*224) + (threadIdx.y*8)) + threadIdx.x), 969)*12544)) + (blockIdx.y*6272)) + (floordiv(floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*224) + (threadIdx.y*8)) + threadIdx.x), 969), 17)*112)) + (blockIdx.x*16)) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*224) + (threadIdx.y*8)) + threadIdx.x), 17)) - 113)], 0f32, dtype=float32)
        }
      }
    }
    attr [IterVar(threadIdx.z_1: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
    attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 28;
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8;
    if @tir.likely((((threadIdx.y_1*8) + threadIdx.x_1) < 36), dtype=bool) {
      if @tir.likely((threadIdx.y_1 < 5), dtype=bool) {
        placeholder.shared[((threadIdx.y_1*8) + threadIdx.x_1)] = (float32*)placeholder_4[(((blockIdx.z*36) + (threadIdx.y_1*8)) + threadIdx.x_1)]
      }
    }
    attr [IterVar(threadIdx.z_2: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
    attr [IterVar(threadIdx.y_2: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 28;
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8 {
      for (ax2: int32, 0, 3) {
        for (ax3: int32, 0, 3) {
          PaddedInput.shared.local[((ax2*3) + ax3)] = (float32*)PaddedInput.shared[((((threadIdx.y_2*34) + (ax2*17)) + (threadIdx.x_2*2)) + ax3)]
          PaddedInput.shared.local[(((ax2*3) + ax3) + 9)] = (float32*)PaddedInput.shared[(((((threadIdx.y_2*34) + (ax2*17)) + (threadIdx.x_2*2)) + ax3) + 969)]
          PaddedInput.shared.local[(((ax2*3) + ax3) + 18)] = (float32*)PaddedInput.shared[(((((threadIdx.y_2*34) + (ax2*17)) + (threadIdx.x_2*2)) + ax3) + 1938)]
          PaddedInput.shared.local[(((ax2*3) + ax3) + 27)] = (float32*)PaddedInput.shared[(((((threadIdx.y_2*34) + (ax2*17)) + (threadIdx.x_2*2)) + ax3) + 2907)]
        }
      }
      for (ax2_1: int32, 0, 3) {
        for (ax3_1: int32, 0, 3) {
          placeholder.shared.local[((ax2_1*3) + ax3_1)] = (float32*)placeholder.shared[((ax2_1*3) + ax3_1)]
          placeholder.shared.local[(((ax2_1*3) + ax3_1) + 9)] = (float32*)placeholder.shared[(((ax2_1*3) + ax3_1) + 9)]
          placeholder.shared.local[(((ax2_1*3) + ax3_1) + 18)] = (float32*)placeholder.shared[(((ax2_1*3) + ax3_1) + 18)]
          placeholder.shared.local[(((ax2_1*3) + ax3_1) + 27)] = (float32*)placeholder.shared[(((ax2_1*3) + ax3_1) + 27)]
        }
      }
      DepthwiseConv2d[0] = 0f32
      DepthwiseConv2d[1] = 0f32
      DepthwiseConv2d[2] = 0f32
      DepthwiseConv2d[3] = 0f32
      for (di: int32, 0, 3) {
        for (dj: int32, 0, 3) {
          DepthwiseConv2d[0] = ((float32*)DepthwiseConv2d[0] + ((float32*)PaddedInput.shared.local[((di*3) + dj)]*(float32*)placeholder.shared.local[((di*3) + dj)]))
          DepthwiseConv2d[1] = ((float32*)DepthwiseConv2d[1] + ((float32*)PaddedInput.shared.local[(((di*3) + dj) + 9)]*(float32*)placeholder.shared.local[(((di*3) + dj) + 9)]))
          DepthwiseConv2d[2] = ((float32*)DepthwiseConv2d[2] + ((float32*)PaddedInput.shared.local[(((di*3) + dj) + 18)]*(float32*)placeholder.shared.local[(((di*3) + dj) + 18)]))
          DepthwiseConv2d[3] = ((float32*)DepthwiseConv2d[3] + ((float32*)PaddedInput.shared.local[(((di*3) + dj) + 27)]*(float32*)placeholder.shared.local[(((di*3) + dj) + 27)]))
        }
      }
      T_relu_2[(((((blockIdx.z*12544) + (blockIdx.y*1568)) + (threadIdx.y_2*56)) + (blockIdx.x*8)) + threadIdx.x_2)] = max((float32*)DepthwiseConv2d[0], 0f32)
      T_relu_2[((((((blockIdx.z*12544) + (blockIdx.y*1568)) + (threadIdx.y_2*56)) + (blockIdx.x*8)) + threadIdx.x_2) + 3136)] = max((float32*)DepthwiseConv2d[1], 0f32)
      T_relu_2[((((((blockIdx.z*12544) + (blockIdx.y*1568)) + (threadIdx.y_2*56)) + (blockIdx.x*8)) + threadIdx.x_2) + 6272)] = max((float32*)DepthwiseConv2d[2], 0f32)
      T_relu_2[((((((blockIdx.z*12544) + (blockIdx.y*1568)) + (threadIdx.y_2*56)) + (blockIdx.x*8)) + threadIdx.x_2) + 9408)] = max((float32*)DepthwiseConv2d[3], 0f32)
    }
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu_12
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 144, 112, 112], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [144, 24, 1, 1], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 24, 112, 112], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [y: Pointer(float32)] "storage_scope" = "global";
  allocate(y, float32, [1806336]) {
    attr [0] "extern_scope" = 0;
    @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 0, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_5, @tir.tvm_stack_make_shape(1, 24, 112, 112, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_4, @tir.tvm_stack_make_shape(144, 24, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y, @tir.tvm_stack_make_shape(1, 144, 112, 112, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 1764;
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 1024;
    T_relu_2[((blockIdx.x*1024) + threadIdx.x)] = max((float32*)y[((blockIdx.x*1024) + threadIdx.x)], 0f32)
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
add, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_add_4
primfn(placeholder_3: handle, placeholder_4: handle, placeholder_5: handle, T_add_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {placeholder_2: Buffer(placeholder_6: Pointer(float32), float32, [1, 24, 112, 112], []),
             T_add: Buffer(T_add_2: Pointer(float32), float32, [1, 24, 112, 112], []),
             placeholder_1: Buffer(placeholder_7: Pointer(float32), float32, [24, 144, 1, 1], []),
             placeholder: Buffer(placeholder_8: Pointer(float32), float32, [1, 144, 112, 112], [])}
  buffer_map = {placeholder_3: placeholder, placeholder_4: placeholder_1, placeholder_5: placeholder_2, T_add_1: T_add} {
  attr [y: Pointer(float32)] "storage_scope" = "global";
  allocate(y, float32, [301056]) {
    attr [0] "extern_scope" = 0;
    @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 1, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_8, @tir.tvm_stack_make_shape(1, 144, 112, 112, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_7, @tir.tvm_stack_make_shape(24, 144, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y, @tir.tvm_stack_make_shape(1, 24, 112, 112, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 294;
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 1024;
    T_add_2[((blockIdx.x*1024) + threadIdx.x)] = ((float32*)y[((blockIdx.x*1024) + threadIdx.x)] + (float32*)placeholder_6[((blockIdx.x*1024) + threadIdx.x)])
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> depthwise_conv2d_nchw.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu_13
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 144, 112, 112], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [144, 1, 3, 3], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 144, 112, 112], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [IterVar(blockIdx.z: int32, (nullptr), "ThreadIndex", "blockIdx.z")] "thread_extent" = 36;
  attr [PaddedInput.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(PaddedInput.shared, float32, [1200]);
  attr [placeholder.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(placeholder.shared, float32, [36]);
  attr [PaddedInput.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(PaddedInput.shared.local, float32, [36]);
  attr [placeholder.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(placeholder.shared.local, float32, [36]);
  attr [DepthwiseConv2d: Pointer(float32)] "storage_scope" = "local";
  allocate(DepthwiseConv2d, float32, [4]);
  attr [IterVar(blockIdx.y: int32, (nullptr), "ThreadIndex", "blockIdx.y")] "thread_extent" = 4;
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 14 {
    for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 6) {
      attr [IterVar(threadIdx.z: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
      attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 28;
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8;
      if @tir.likely(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*224) + (threadIdx.y*8)) + threadIdx.x) < 1200), dtype=bool) {
        if @tir.likely((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*28) + threadIdx.y) < 150), dtype=bool) {
          PaddedInput.shared[(((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*224) + (threadIdx.y*8)) + threadIdx.x)] = @tir.if_then_else(((((1 <= ((blockIdx.y*28) + floordiv(floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*224) + (threadIdx.y*8)) + threadIdx.x), 300), 10))) && (((blockIdx.y*28) + floordiv(floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*224) + (threadIdx.y*8)) + threadIdx.x), 300), 10)) < 113)) && (1 <= ((blockIdx.x*8) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*224) + (threadIdx.y*8)) + threadIdx.x), 10)))) && (((blockIdx.x*8) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*224) + (threadIdx.y*8)) + threadIdx.x), 10)) < 113)), (float32*)placeholder_5[(((((((blockIdx.z*50176) + (floordiv((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*224) + (threadIdx.y*8)) + threadIdx.x), 300)*12544)) + (blockIdx.y*3136)) + (floordiv(floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*224) + (threadIdx.y*8)) + threadIdx.x), 300), 10)*112)) + (blockIdx.x*8)) + floormod((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*224) + (threadIdx.y*8)) + threadIdx.x), 10)) - 113)], 0f32, dtype=float32)
        }
      }
    }
    attr [IterVar(threadIdx.z_1: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
    attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 28;
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8;
    if @tir.likely((((threadIdx.y_1*8) + threadIdx.x_1) < 36), dtype=bool) {
      if @tir.likely((threadIdx.y_1 < 5), dtype=bool) {
        placeholder.shared[((threadIdx.y_1*8) + threadIdx.x_1)] = (float32*)placeholder_4[(((blockIdx.z*36) + (threadIdx.y_1*8)) + threadIdx.x_1)]
      }
    }
    attr [IterVar(threadIdx.z_2: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
    attr [IterVar(threadIdx.y_2: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 28;
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 8 {
      for (ax2: int32, 0, 3) {
        for (ax3: int32, 0, 3) {
          PaddedInput.shared.local[((ax2*3) + ax3)] = (float32*)PaddedInput.shared[((((ax2*10) + (threadIdx.y_2*10)) + ax3) + threadIdx.x_2)]
          PaddedInput.shared.local[(((ax2*3) + ax3) + 9)] = (float32*)PaddedInput.shared[(((((ax2*10) + (threadIdx.y_2*10)) + ax3) + threadIdx.x_2) + 300)]
          PaddedInput.shared.local[(((ax2*3) + ax3) + 18)] = (float32*)PaddedInput.shared[(((((ax2*10) + (threadIdx.y_2*10)) + ax3) + threadIdx.x_2) + 600)]
          PaddedInput.shared.local[(((ax2*3) + ax3) + 27)] = (float32*)PaddedInput.shared[(((((ax2*10) + (threadIdx.y_2*10)) + ax3) + threadIdx.x_2) + 900)]
        }
      }
      for (ax2_1: int32, 0, 3) {
        for (ax3_1: int32, 0, 3) {
          placeholder.shared.local[((ax2_1*3) + ax3_1)] = (float32*)placeholder.shared[((ax2_1*3) + ax3_1)]
          placeholder.shared.local[(((ax2_1*3) + ax3_1) + 9)] = (float32*)placeholder.shared[(((ax2_1*3) + ax3_1) + 9)]
          placeholder.shared.local[(((ax2_1*3) + ax3_1) + 18)] = (float32*)placeholder.shared[(((ax2_1*3) + ax3_1) + 18)]
          placeholder.shared.local[(((ax2_1*3) + ax3_1) + 27)] = (float32*)placeholder.shared[(((ax2_1*3) + ax3_1) + 27)]
        }
      }
      DepthwiseConv2d[0] = 0f32
      DepthwiseConv2d[1] = 0f32
      DepthwiseConv2d[2] = 0f32
      DepthwiseConv2d[3] = 0f32
      for (di: int32, 0, 3) {
        for (dj: int32, 0, 3) {
          DepthwiseConv2d[0] = ((float32*)DepthwiseConv2d[0] + ((float32*)PaddedInput.shared.local[((di*3) + dj)]*(float32*)placeholder.shared.local[((di*3) + dj)]))
          DepthwiseConv2d[1] = ((float32*)DepthwiseConv2d[1] + ((float32*)PaddedInput.shared.local[(((di*3) + dj) + 9)]*(float32*)placeholder.shared.local[(((di*3) + dj) + 9)]))
          DepthwiseConv2d[2] = ((float32*)DepthwiseConv2d[2] + ((float32*)PaddedInput.shared.local[(((di*3) + dj) + 18)]*(float32*)placeholder.shared.local[(((di*3) + dj) + 18)]))
          DepthwiseConv2d[3] = ((float32*)DepthwiseConv2d[3] + ((float32*)PaddedInput.shared.local[(((di*3) + dj) + 27)]*(float32*)placeholder.shared.local[(((di*3) + dj) + 27)]))
        }
      }
      T_relu_2[(((((blockIdx.z*50176) + (blockIdx.y*3136)) + (threadIdx.y_2*112)) + (blockIdx.x*8)) + threadIdx.x_2)] = max((float32*)DepthwiseConv2d[0], 0f32)
      T_relu_2[((((((blockIdx.z*50176) + (blockIdx.y*3136)) + (threadIdx.y_2*112)) + (blockIdx.x*8)) + threadIdx.x_2) + 12544)] = max((float32*)DepthwiseConv2d[1], 0f32)
      T_relu_2[((((((blockIdx.z*50176) + (blockIdx.y*3136)) + (threadIdx.y_2*112)) + (blockIdx.x*8)) + threadIdx.x_2) + 25088)] = max((float32*)DepthwiseConv2d[2], 0f32)
      T_relu_2[((((((blockIdx.z*50176) + (blockIdx.y*3136)) + (threadIdx.y_2*112)) + (blockIdx.x*8)) + threadIdx.x_2) + 37632)] = max((float32*)DepthwiseConv2d[3], 0f32)
    }
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
lower function fused_nn_conv2d_5
primfn(placeholder_2: handle, placeholder_3: handle, y_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {y: Buffer(y_2: Pointer(float32), float32, [1, 24, 112, 112], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [24, 96, 1, 1], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 96, 112, 112], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, y_1: y} {
  attr [0] "extern_scope" = 0;
  @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 1, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_5, @tir.tvm_stack_make_shape(1, 96, 112, 112, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_4, @tir.tvm_stack_make_shape(24, 96, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y_2, @tir.tvm_stack_make_shape(1, 24, 112, 112, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> depthwise_conv2d_nchw.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu_14
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 96, 112, 112], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [96, 1, 3, 3], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 96, 224, 224], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [IterVar(blockIdx.z: int32, (nullptr), "ThreadIndex", "blockIdx.z")] "thread_extent" = 96;
  attr [PaddedInput.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(PaddedInput.shared, float32, [1921]);
  attr [placeholder.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(placeholder.shared, float32, [9]);
  attr [PaddedInput.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(PaddedInput.shared.local, float32, [102]);
  attr [placeholder.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(placeholder.shared.local, float32, [9]);
  attr [DepthwiseConv2d: Pointer(float32)] "storage_scope" = "local";
  allocate(DepthwiseConv2d, float32, [16]);
  attr [IterVar(blockIdx.y: int32, (nullptr), "ThreadIndex", "blockIdx.y")] "thread_extent" = 14;
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 2 {
    for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 69) "unroll" {
      attr [IterVar(threadIdx.z: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
      attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 1;
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28;
      if @tir.likely((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*28) + threadIdx.x) < 1921), dtype=bool) {
        PaddedInput.shared[((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*28) + threadIdx.x)] = @tir.if_then_else(((1 <= ((blockIdx.y*16) + floordiv(((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*28) + threadIdx.x), 113))) && (1 <= ((blockIdx.x*112) + floormod(((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*28) + threadIdx.x), 113)))), (float32*)placeholder_5[((((((blockIdx.z*50176) + (blockIdx.y*3584)) + (floordiv(((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*28) + threadIdx.x), 113)*224)) + (blockIdx.x*112)) + floormod(((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*28) + threadIdx.x), 113)) - 225)], 0f32, dtype=float32)
      }
    }
    attr [IterVar(threadIdx.z_1: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
    attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 1;
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28;
    if @tir.likely((threadIdx.x_1 < 9), dtype=bool) {
      placeholder.shared[threadIdx.x_1] = (float32*)placeholder_4[((blockIdx.z*9) + threadIdx.x_1)]
    }
    attr [IterVar(threadIdx.z_2: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 1;
    attr [IterVar(threadIdx.y_2: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 1;
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
      for (ax2: int32, 0, 17) "unroll" {
        for (ax3: int32, 0, 3) "unroll" {
          PaddedInput.shared.local[((ax2*3) + ax3)] = (float32*)PaddedInput.shared[(((ax2*113) + (threadIdx.x_2*2)) + ax3)]
          PaddedInput.shared.local[(((ax2*3) + ax3) + 51)] = (float32*)PaddedInput.shared[((((ax2*113) + (threadIdx.x_2*2)) + ax3) + 56)]
        }
      }
      for (ax2_1: int32, 0, 3) "unroll" {
        for (ax3_1: int32, 0, 3) "unroll" {
          placeholder.shared.local[((ax2_1*3) + ax3_1)] = (float32*)placeholder.shared[((ax2_1*3) + ax3_1)]
        }
      }
      for (i: int32, 0, 8) "unroll" {
        DepthwiseConv2d[i] = 0f32
        DepthwiseConv2d[(i + 8)] = 0f32
        for (di: int32, 0, 3) "unroll" {
          for (dj: int32, 0, 3) "unroll" {
            DepthwiseConv2d[i] = ((float32*)DepthwiseConv2d[i] + ((float32*)PaddedInput.shared.local[(((i*6) + (di*3)) + dj)]*(float32*)placeholder.shared.local[((di*3) + dj)]))
            DepthwiseConv2d[(i + 8)] = ((float32*)DepthwiseConv2d[(i + 8)] + ((float32*)PaddedInput.shared.local[((((i*6) + (di*3)) + dj) + 51)]*(float32*)placeholder.shared.local[((di*3) + dj)]))
          }
        }
      }
      for (ax2.inner.inner.inner: int32, 0, 8) "unroll" {
        T_relu_2[(((((blockIdx.z*12544) + (blockIdx.y*896)) + (ax2.inner.inner.inner*112)) + (blockIdx.x*56)) + threadIdx.x_2)] = max((float32*)DepthwiseConv2d[ax2.inner.inner.inner], 0f32)
        T_relu_2[((((((blockIdx.z*12544) + (blockIdx.y*896)) + (ax2.inner.inner.inner*112)) + (blockIdx.x*56)) + threadIdx.x_2) + 28)] = max((float32*)DepthwiseConv2d[(ax2.inner.inner.inner + 8)], 0f32)
      }
    }
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu_15
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 96, 224, 224], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [96, 16, 1, 1], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 16, 224, 224], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [y: Pointer(float32)] "storage_scope" = "global";
  allocate(y, float32, [4816896]) {
    attr [0] "extern_scope" = 0;
    @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 1, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_5, @tir.tvm_stack_make_shape(1, 16, 224, 224, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_4, @tir.tvm_stack_make_shape(96, 16, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y, @tir.tvm_stack_make_shape(1, 96, 224, 224, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
    attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 4704;
    attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 1024;
    T_relu_2[((blockIdx.x*1024) + threadIdx.x)] = max((float32*)y[((blockIdx.x*1024) + threadIdx.x)], 0f32)
  }
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> conv2d_cudnn.cuda
lower function fused_nn_conv2d_6
primfn(placeholder_2: handle, placeholder_3: handle, y_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {y: Buffer(y_2: Pointer(float32), float32, [1, 16, 224, 224], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [16, 32, 1, 1], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 32, 224, 224], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, y_1: y} {
  attr [0] "extern_scope" = 0;
  @tir.tvm_call_packed("tvm.contrib.cudnn.conv2d.forward", 1, 0, 0, 0, 0, 1, 1, 1, 1, @tir.tvm_stack_make_array(placeholder_5, @tir.tvm_stack_make_shape(1, 32, 224, 224, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(placeholder_4, @tir.tvm_stack_make_shape(16, 32, 1, 1, dtype=handle), 0, 4, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(y_2, @tir.tvm_stack_make_shape(1, 16, 224, 224, dtype=handle), 0, 4, 0f32, 0, dtype=handle), "float32", 1, dtype=int32)
}


nn.conv2d, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> depthwise_conv2d_nchw.cuda
nn.relu, cuda -keys=cuda,gpu -libs=cudnn -max_num_threads=1024 -thread_warp_size=32 --> injective.cuda
lower function fused_nn_conv2d_nn_relu_16
primfn(placeholder_2: handle, placeholder_3: handle, T_relu_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {T_relu: Buffer(T_relu_2: Pointer(float32), float32, [1, 32, 224, 224], []),
             placeholder_1: Buffer(placeholder_4: Pointer(float32), float32, [32, 1, 3, 3], []),
             placeholder: Buffer(placeholder_5: Pointer(float32), float32, [1, 32, 224, 224], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_relu_1: T_relu} {
  attr [IterVar(blockIdx.z: int32, (nullptr), "ThreadIndex", "blockIdx.z")] "thread_extent" = 16;
  attr [PaddedInput.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(PaddedInput.shared, float32, [576]);
  attr [placeholder.shared: Pointer(float32)] "storage_scope" = "shared";
  allocate(placeholder.shared, float32, [18]);
  attr [PaddedInput.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(PaddedInput.shared.local, float32, [32]);
  attr [placeholder.shared.local: Pointer(float32)] "storage_scope" = "local";
  allocate(placeholder.shared.local, float32, [9]);
  attr [DepthwiseConv2d: Pointer(float32)] "storage_scope" = "local";
  allocate(DepthwiseConv2d, float32, [8]);
  attr [IterVar(blockIdx.y: int32, (nullptr), "ThreadIndex", "blockIdx.y")] "thread_extent" = 14;
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 16 {
    for (ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer: int32, 0, 11) "unroll" {
      attr [IterVar(threadIdx.z: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 2;
      attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 7;
      if @tir.likely((((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*56) + (threadIdx.z*28)) + (threadIdx.y*7)) + threadIdx.x) < 576), dtype=bool) {
        if @tir.likely(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*8) + (threadIdx.z*4)) + threadIdx.y) < 83), dtype=bool) {
          if @tir.likely((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*2) + threadIdx.z) < 21), dtype=bool) {
            PaddedInput.shared[((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*56) + (threadIdx.z*28)) + (threadIdx.y*7)) + threadIdx.x)] = @tir.if_then_else(((((1 <= ((blockIdx.y*16) + floordiv(floormod(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*56) + (threadIdx.z*28)) + (threadIdx.y*7)) + threadIdx.x), 288), 16))) && (((blockIdx.y*16) + floordiv(floormod(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*56) + (threadIdx.z*28)) + (threadIdx.y*7)) + threadIdx.x), 288), 16)) < 225)) && (1 <= ((blockIdx.x*14) + floormod(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*56) + (threadIdx.z*28)) + (threadIdx.y*7)) + threadIdx.x), 16)))) && (((blockIdx.x*14) + floormod(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*56) + (threadIdx.z*28)) + (threadIdx.y*7)) + threadIdx.x), 16)) < 225)), (float32*)placeholder_5[(((((((blockIdx.z*100352) + (floordiv(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*56) + (threadIdx.z*28)) + (threadIdx.y*7)) + threadIdx.x), 288)*50176)) + (blockIdx.y*3584)) + (floordiv(floormod(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*56) + (threadIdx.z*28)) + (threadIdx.y*7)) + threadIdx.x), 288), 16)*224)) + (blockIdx.x*14)) + floormod(((((ax0.ax1.fused.ax2.fused.ax3.fused.outer.outer.outer*56) + (threadIdx.z*28)) + (threadIdx.y*7)) + threadIdx.x), 16)) - 225)], 0f32, dtype=float32)
          }
        }
      }
    }
    attr [IterVar(threadIdx.z_1: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 2;
    attr [IterVar(threadIdx.y_1: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;
    attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 7;
    if @tir.likely(((((threadIdx.z_1*28) + (threadIdx.y_1*7)) + threadIdx.x_1) < 18), dtype=bool) {
      if @tir.likely((((threadIdx.z_1*4) + threadIdx.y_1) < 3), dtype=bool) {
        if @tir.likely((threadIdx.z_1 < 1), dtype=bool) {
          placeholder.shared[(((threadIdx.z_1*28) + (threadIdx.y_1*7)) + threadIdx.x_1)] = (float32*)placeholder_4[((((threadIdx.z_1*28) + (blockIdx.z*18)) + (threadIdx.y_1*7)) + threadIdx.x_1)]
        }
      }
    }
    attr [IterVar(threadIdx.z_2: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 2;
    attr [IterVar(threadIdx.y_2: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;
    attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 7 {
      for (ax2: int32, 0, 4) "unroll" {
        for (ax3: int32, 0, 4) "unroll" {
          PaddedInput.shared.local[((ax2*4) + ax3)] = (float32*)PaddedInput.shared[(((((threadIdx.z_2*288) + (threadIdx.y_2*32)) + (ax2*16)) + (threadIdx.x_2*2)) + ax3)]
          PaddedInput.shared.local[(((ax2*4) + ax3) + 16)] = (float32*)PaddedInput.shared[((((((threadIdx.z_2*288) + (threadIdx.y_2*32)) + (ax2*16)) + (threadIdx.x_2*2)) + ax3) + 128)]
        }
      }
      for (ax2_1: int32, 0, 3) "unroll" {
        for (ax3_1: int32, 0, 3) "unroll" {
          placeholder.shared.local[((ax2_1*3) + ax3_1)] = (float32*)placeholder.shared[(((threadIdx.z_2*9) + (ax2_1*3)) + ax3_1)]
        }
      }
      for (i: int32, 0, 2) "unroll" {
        for (j: int32, 0, 2) "unroll" {
          DepthwiseConv2d[((i*2) + j)] = 0f32
          DepthwiseConv2d[(((i*2) + j) + 4)] = 0f32
          for (di: int32, 0, 3) "unroll" {
            for (dj: int32, 0, 3) "unroll" {
              DepthwiseConv2d[((i*2) + j)] = ((float32*)DepthwiseConv2d[((i*2) + j)] + ((float32*)PaddedInput.shared.local[((((i*4) + (di*4)) + j) + dj)]*(float32*)placeholder.shared.local[((di*3) + dj)]))
              DepthwiseConv2d[(((i*2) + j) + 4)] = ((float32*)DepthwiseConv2d[(((i*2) + j) + 4)] + ((float32*)PaddedInput.shared.local[(((((i*4) + (di*4)) + j) + dj) + 16)]*(float32*)placeholder.shared.local[((di*3) + dj)]))
            }
          }
        }
      }
      for (ax2.inner.inner.inner: int32, 0, 2) "unroll" {
        for (ax3.inner.inner.inner: int32, 0, 2) "unroll" {
          T_relu_2[((((((((blockIdx.z*100352) + (threadIdx.z_2*50176)) + (blockIdx.y*3584)) + (threadIdx.y_2*448)) + (ax2.inner.inner.inner*224)) + (blockIdx.x*14)) + (threadIdx.x_2*2)) + ax3.inner.inner.inner)] = max((float32*)DepthwiseConv2d[((ax2.inner.inner.inner*2) + ax3.inner.inner.inner)], 0f32)
          T_relu_2[(((((((((blockIdx.z*100352) + (threadIdx.z_2*50176)) + (blockIdx.y*3584)) + (threadIdx.y_2*448)) + (ax2.inner.inner.inner*224)) + (blockIdx.x*14)) + (threadIdx.x_2*2)) + ax3.inner.inner.inner) + 1792)] = max((float32*)DepthwiseConv2d[(((ax2.inner.inner.inner*2) + ax3.inner.inner.inner) + 4)], 0f32)
        }
      }
    }
  }
}


Mean, std of perf : 2.0534602333333334, 0.3341073741446262
Mean, std of perf : 2.3011029333333335, 0.6504474043205957
Mean, std of perf : 2.6114560166666667, 0.3934652457433742
Mean, std of perf : 2.364791466666667, 0.6423244380876594
Mean, std of perf : 2.053887466666667, 0.31815621388827303
Mean, std of perf : 2.330829283333333, 0.6361501917061518
Mean, std of perf : 2.6145300333333332, 0.396695259615651
Traceback (most recent call last):
  File "testing/measure_end_to_end.py", line 211, in <module>
    mean_perf, std_perf = measure_end_to_end_perf_cudnn(mod["main"], params, 'cuda -libs=cudnn', shape_dict,
  File "testing/measure_end_to_end.py", line 84, in measure_end_to_end_perf_cudnn
    return measure(ftimer, is_net=False)
  File "/home/sunggg/tvm/python/tvm/relay/transform/backend_operator/target.py", line 77, in measure
    perfs = np.array(ftimer(*args).results) * 1000  # convert to millisecond
  File "/home/sunggg/tvm/python/tvm/runtime/module.py", line 229, in evaluator
    blob = feval(*args)
  File "/home/sunggg/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 227, in __call__
    _LIB.TVMFuncCall(
KeyboardInterrupt
